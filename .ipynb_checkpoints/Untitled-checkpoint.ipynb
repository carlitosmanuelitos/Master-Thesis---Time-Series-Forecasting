{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cba202-156b-4503-a160-4b30285aa4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import warnings\n",
    "import logging\n",
    "import pickle\n",
    "import hashlib\n",
    "from abc import abstractmethod\n",
    "from datetime import datetime, timedelta\n",
    "from math import pi\n",
    "from typing import Optional\n",
    "from joblib import dump, load\n",
    "\n",
    "# Third-party imports\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from scipy.signal import detrend\n",
    "from scipy.stats import jarque_bera, kstest\n",
    "#from scipy.stats import boxcox, invboxcox  # Uncomment if needed\n",
    "from cryptocmd import CmcScraper\n",
    "from prettytable import PrettyTable\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "from arch import arch_model\n",
    "\n",
    "# Plotting and Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from bokeh.plotting import figure, show, output_notebook, save\n",
    "from bokeh.models import (HoverTool, ColumnDataSource, WheelZoomTool, Span, Range1d,\n",
    "                          FreehandDrawTool, MultiLine, NumeralTickFormatter, Button, CustomJS)\n",
    "from bokeh.layouts import column, row\n",
    "from bokeh.io import curdoc, export_png\n",
    "from bokeh.models.widgets import CheckboxGroup\n",
    "from bokeh.themes import Theme\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Machine Learning Libraries\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, cross_val_score\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, QuantileTransformer, PowerTransformer\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score, accuracy_score\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.svm import SVR\n",
    "from lightgbm import LGBMRegressor\n",
    "import xgboost as xgb\n",
    "from statsmodels.tsa.api import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller, grangercausalitytests, kpss\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "# Deep Learning Libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential, Model\n",
    "from tensorflow.keras.layers import (Dense, Dropout, LSTM, TimeDistributed, Conv1D, MaxPooling1D, Flatten,\n",
    "                                    ConvLSTM2D, BatchNormalization, GRU, Bidirectional, Attention, Input,\n",
    "                                    Reshape, GlobalAveragePooling1D, GlobalMaxPooling1D, Lambda, LayerNormalization, \n",
    "                                    SimpleRNN, Layer, Multiply, Add, Activation)\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
    "from keras_tuner import HyperModel, RandomSearch, BayesianOptimization\n",
    "from tcn import TCN\n",
    "from kerasbeats import NBeatsModel\n",
    "from typing import List, Optional\n",
    "from typing import Optional, List, Tuple\n",
    "from statsmodels.tsa.stattools import acf, pacf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b682977-5500-45c5-8da6-d9c7c3749729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other settings\n",
    "from IPython.display import display, HTML\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.float_format', '{:.3f}'.format)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "display(HTML(\"<style>.bk-root { margin-left: auto; margin-right: auto; }</style>\"))\n",
    "\n",
    "print(\"Hello Everyone\")\n",
    "os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'\n",
    "print(f\"Tensor Flow Version: {tf.__version__}\")\n",
    "print(f\"Keras Version: {tf.__version__}\")\n",
    "print()\n",
    "print(f\"Python {sys.version}\")\n",
    "print(f\"Pandas {pd.__version__}\")\n",
    "print(f\"Scikit-Learn {sklearn.__version__}\")\n",
    "gpu = len(tf.config.list_physical_devices('GPU'))>0\n",
    "print(\"GPU is\", \"available\" if gpu else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79330df-2aa4-446d-b742-93918050ce30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CryptoData:\n",
    "    \"\"\"\n",
    "    The CryptoData class is responsible for fetching and validating cryptocurrency data. \n",
    "    It provides methods to fetch raw data, validate its integrity, and format it for display.\n",
    "    \n",
    "    Attributes:\n",
    "    - EXPECTED_COLUMNS: A set of expected columns in the fetched data.\n",
    "    - crypto_symbols: A list of cryptocurrency symbols to fetch.\n",
    "    - retries: The maximum number of data fetch retries.\n",
    "    - backoff_factor: The exponential backoff factor for retries.\n",
    "    \"\"\"\n",
    "    \n",
    "    EXPECTED_COLUMNS = {'Date', 'Open', 'High', 'Low', 'Close', 'Market Cap', 'Volume'}\n",
    "\n",
    "    def __init__(self, crypto_symbols: list[str], retries: int = 5, backoff_factor: float = 0.3):\n",
    "        \"\"\"Initializes the class with the given list of cryptocurrency symbols.\"\"\"\n",
    "        logger.info(\"Initializing CryptoData class.\")\n",
    "        self.crypto_symbols = crypto_symbols\n",
    "        self.retries = retries\n",
    "        self.backoff_factor = backoff_factor\n",
    "        self.DATA_DIR = \"crypto_assets\"\n",
    "        if not os.path.exists(self.DATA_DIR):\n",
    "            os.makedirs(self.DATA_DIR)\n",
    "        logger.info(\"CryptoData class initialized.\")\n",
    "\n",
    "    def _fetch_cryptocmd_data(self, symbol: str) -> pd.DataFrame:\n",
    "        \"\"\"Fetches cryptocurrency data with retries and exponential backoff.\"\"\"\n",
    "        logger.info(f\"Fetching data for {symbol}.\")\n",
    "        scraper = CmcScraper(symbol)\n",
    "        df = scraper.get_dataframe()\n",
    "\n",
    "        # Drop unwanted columns\n",
    "        unwanted_columns = ['Time Open', 'Time High', 'Time Low', 'Time Close']\n",
    "        df.drop(columns=unwanted_columns, inplace=True)\n",
    "        \n",
    "        # Sort data by Date in ascending order\n",
    "        df.sort_values(by='Date', ascending=True, inplace=True)\n",
    "        return df\n",
    "\n",
    "    def _local_data_path(self, symbol: str) -> str:\n",
    "        return os.path.join(self.DATA_DIR, f\"data_c_{symbol}.csv\")\n",
    "\n",
    "    def get_cryptocmd_data(self, symbol: str, overwrite: bool = False) -> pd.DataFrame:\n",
    "        \"\"\"Fetches and returns the cryptocurrency data.\"\"\"\n",
    "        logger.info(f\"Retrieving {symbol} data.\")\n",
    "        df = self._fetch_cryptocmd_data(symbol)\n",
    "        \n",
    "        # Save to local storage if needed\n",
    "        file_path = self._local_data_path(symbol)\n",
    "        if overwrite or not os.path.exists(file_path):\n",
    "            df.to_csv(file_path, index=False)\n",
    "        \n",
    "        # Set 'Date' as the index\n",
    "        df.set_index('Date', inplace=True)\n",
    "        return df\n",
    "\n",
    "    def get_all_data(self, overwrite: bool = False) -> dict[str, pd.DataFrame]:\n",
    "        \"\"\"Fetches data for all specified cryptocurrencies.\"\"\"\n",
    "        logger.info(\"Getting data for all specified cryptocurrencies.\")\n",
    "        data_dict = {}\n",
    "        for symbol in self.crypto_symbols:\n",
    "            data_dict[symbol] = self.get_cryptocmd_data(symbol, overwrite)\n",
    "        logger.info(\"All data retrieved successfully.\")\n",
    "        return data_dict\n",
    "\n",
    "    @staticmethod\n",
    "    def _format_monetary_value(value: float) -> str:\n",
    "        \"\"\"Formats a monetary value to a string.\"\"\"\n",
    "        return \"${:,.2f}\".format(value)\n",
    "\n",
    "    @staticmethod\n",
    "    def _format_volume_value(value: float) -> str:\n",
    "        \"\"\"Formats a volume value to a string.\"\"\"\n",
    "        if value > 1e9:\n",
    "            return \"{:.2f}B\".format(value/1e9)\n",
    "        elif value > 1e6:\n",
    "            return \"{:.2f}M\".format(value/1e6)\n",
    "        else:\n",
    "            return \"{:,.2f}\".format(value)\n",
    "\n",
    "    def get_display_data(self, symbol: str) -> pd.DataFrame:\n",
    "        \"\"\"Formats the cryptocurrency data for display.\"\"\"\n",
    "        logger.info(f\"Formatting display data for {symbol}.\")\n",
    "        \n",
    "        # Load the data for the given symbol from local storage\n",
    "        file_path = self._local_data_path(symbol)\n",
    "        if not os.path.exists(file_path):\n",
    "            raise ValueError(f\"No data found for {symbol}. Please fetch the data first.\")\n",
    "        display_df = pd.read_csv(file_path, parse_dates=['Date']).set_index('Date')\n",
    "        \n",
    "        # Format the data\n",
    "        monetary_columns = ['Open', 'High', 'Low', 'Close']\n",
    "        display_df[monetary_columns] = display_df[monetary_columns].applymap(self._format_monetary_value)\n",
    "        volume_like_columns = ['Volume', 'Market Cap']\n",
    "        display_df[volume_like_columns] = display_df[volume_like_columns].applymap(self._format_volume_value)\n",
    "        logger.info(f\"Display data formatted successfully for {symbol}.\")\n",
    "        return display_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e911154-3603-4cfc-8299-cb8fe30dcb38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "crypto_data_obj = CryptoData(['BTC', 'ETH', 'ADA'])\n",
    "all_data = crypto_data_obj.get_all_data(overwrite=True)\n",
    "btc_data, eth_data, ada_data = all_data['BTC'], all_data['ETH'], all_data['ADA']\n",
    "btc_display_data = crypto_data_obj.get_display_data('BTC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5861d15d-e675-48d4-b419-11da31ab3045",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = btc_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faad627-243f-4537-89ea-b3c82af28afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnifiedDataPreprocessor:\n",
    "    \"\"\" \n",
    "    UnifiedDataPreprocessor is responsible for preprocessing time series data.\n",
    "    It performs actions like data splitting, normalization, reshaping, and sequence generation.\n",
    "    \n",
    "    Attributes:\n",
    "        data (pd.DataFrame): Original time series data.\n",
    "        target_column (str): Target column for preprocessing.\n",
    "        logger (logging.Logger): Logger for tracking operations and debugging.\n",
    "        transformations (list): List of applied transformations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df, target_column, logger=None):\n",
    "        self.data = df.copy()\n",
    "        self.target_column = target_column\n",
    "        self.scalers = {}\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = None, None, None, None\n",
    "        self.X_train_seq, self.X_test_seq, self.y_train_seq, self.y_test_seq = None, None, None, None\n",
    "        self.logger = logger if logger else logging.getLogger(__name__)\n",
    "        self.transformations = []\n",
    "        self.lambda_val = None  \n",
    "        self.scalers = {\n",
    "            \"MinMax\": MinMaxScaler(),\n",
    "            \"Standard\": StandardScaler(),\n",
    "            \"Robust\": RobustScaler(),\n",
    "            \"Quantile\": QuantileTransformer(output_distribution='normal'),\n",
    "            \"Power\": PowerTransformer(method='yeo-johnson')\n",
    "        }\n",
    "        self.logger.info(\"Initializing DataPreprocessor...\")        \n",
    "    \n",
    "    def get_scaler(self, scaler_type: str):\n",
    "        self.logger.info(f\"Getting scaler of type: {scaler_type}\")\n",
    "        try:\n",
    "            return self.scalers[scaler_type]\n",
    "        except KeyError:\n",
    "            raise ValueError(f\"Invalid scaler_type. Supported types are: {', '.join(self.scalers.keys())}\")\n",
    "\n",
    "    def split_and_plot_data(self, test_size: float = 0.2, split_date: Optional[str] = None, plot: bool = True):\n",
    "        self.logger.info(\"Splitting data...\")\n",
    "        self.transformations.append('Data Splitting')\n",
    "        features = self.data.drop(columns=[self.target_column])\n",
    "        target = self.data[self.target_column]\n",
    "\n",
    "        if split_date:\n",
    "            train_mask = self.data.index < split_date\n",
    "            self.X_train, self.X_test = features[train_mask], features[~train_mask]\n",
    "            self.y_train, self.y_test = target[train_mask], target[~train_mask]\n",
    "        else:\n",
    "            self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "                features, target, test_size=test_size, shuffle=False\n",
    "            )\n",
    "\n",
    "        self.logger.info(f\"Data split completed. X_train shape: {self.X_train.shape}, y_train shape: {self.y_train.shape}\")\n",
    "\n",
    "        if plot:\n",
    "            plt.figure(figsize=(20, 7))\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.title('Training Data - Target')\n",
    "            plt.plot(self.y_train, label=self.target_column)\n",
    "            plt.xlabel(\"Time\")\n",
    "            plt.ylabel(\"Value\")\n",
    "            plt.legend()\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.title('Test Data - Target')\n",
    "            plt.plot(self.y_test, label=self.target_column)\n",
    "            plt.xlabel(\"Time\")\n",
    "            plt.ylabel(\"Value\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "    def normalize_data(self, scaler_type: str = 'MinMax', plot: bool = True):\n",
    "        self.logger.info(\"Normalizing feature data...\")\n",
    "        scaler = self.get_scaler(scaler_type)\n",
    "        self.X_train = scaler.fit_transform(self.X_train)\n",
    "        self.X_test = scaler.transform(self.X_test)\n",
    "        self.scalers['features'] = scaler\n",
    "        self.logger.info(\"Feature data normalization completed.\")\n",
    "        self.transformations.append(f\"Feature normalization with {scaler_type} scaler\")\n",
    "\n",
    "        if plot:\n",
    "            plt.figure(figsize=(20, 8))\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.title('Normalized Training Features')\n",
    "            for i in range(self.X_train.shape[1]):\n",
    "                plt.plot(self.X_train[:, i], label=f'Feature {i}')\n",
    "            plt.legend()\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.title('Normalized Test Features')\n",
    "            for i in range(self.X_test.shape[1]):\n",
    "                plt.plot(self.X_test[:, i], label=f'Feature {i}')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "    def normalize_target(self, scaler_type: str = 'MinMax', plot: bool = True):\n",
    "        self.logger.info(\"Normalizing target data...\")\n",
    "        scaler = self.get_scaler(scaler_type)\n",
    "        self.y_train = scaler.fit_transform(self.y_train.values.reshape(-1, 1))\n",
    "        self.y_test = scaler.transform(self.y_test.values.reshape(-1, 1))\n",
    "        self.scalers['target'] = scaler\n",
    "        self.logger.info(\"Target data normalization completed.\")\n",
    "        self.transformations.append(f\"Target normalization with {scaler_type} scaler\")\n",
    "\n",
    "        if plot:\n",
    "            plt.figure(figsize=(20, 7))\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.title('Normalized Training Target')\n",
    "            plt.plot(self.y_train, label='Normalized ' + self.target_column)\n",
    "            plt.legend()\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.title('Normalized Test Target')\n",
    "            plt.plot(self.y_test, label='Normalized ' + self.target_column)\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "    def difference_and_plot_data(self, interval: int = 1, plot: bool = True):\n",
    "        self.logger.info(f\"Applying differencing with interval {interval}...\")\n",
    "        self.data = self.data.diff(periods=interval).dropna()\n",
    "        self.transformations.append(f'Differencing with interval {interval}')\n",
    "        self.logger.info(\"Differencing applied.\")\n",
    "        \n",
    "        if plot:\n",
    "            plt.figure(figsize=(20, 7))\n",
    "            plt.title('Data after Differencing')\n",
    "            plt.plot(self.data[self.target_column], label=self.target_column)\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "    def box_cox_transform_and_plot(self, lambda_val: Optional[float] = None, plot: bool = True):\n",
    "        if self.y_train is None or self.y_test is None:\n",
    "            self.logger.warning(\"Data not split yet. Run split_data first.\")\n",
    "            return self  # Allow method chaining\n",
    "\n",
    "        if np.any(self.y_train <= 0) or np.any(self.y_test <= 0):\n",
    "            self.logger.warning(\"Data must be positive for Box-Cox transformation.\")\n",
    "            return self  # Allow method chaining\n",
    "\n",
    "        self.logger.info(\"Applying Box-Cox transformation...\")\n",
    "        self.y_train = self.y_train.ravel()\n",
    "        self.y_test = self.y_test.ravel()\n",
    "        self.y_train, fitted_lambda = boxcox(self.y_train)\n",
    "        self.lambda_val = fitted_lambda if lambda_val is None else lambda_val\n",
    "        self.y_test = boxcox(self.y_test, lmbda=self.lambda_val)\n",
    "        self.transformations.append(f\"Box-Cox transformation with lambda {self.lambda_val}\")\n",
    "        self.logger.info(f\"Box-Cox transformation applied with lambda {self.lambda_val}.\")\n",
    "\n",
    "        if plot:\n",
    "            plt.figure(figsize=(20, 7))\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.title('Box-Cox Transformed Training Target')\n",
    "            plt.plot(self.y_train, label='Transformed ' + self.target_column)\n",
    "            plt.legend()\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.title('Box-Cox Transformed Test Target')\n",
    "            plt.plot(self.y_test, label='Transformed ' + self.target_column)\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "    def inverse_box_cox_and_plot(self, plot: bool = True):\n",
    "        if \"Box-Cox transformation\" not in \"\".join(self.transformations):\n",
    "            self.logger.warning(\"No Box-Cox transformation found on the target column. Skipping inverse transformation.\")\n",
    "            return\n",
    "\n",
    "        self.logger.info(\"Applying inverse Box-Cox transformation...\")\n",
    "        self.y_train = invboxcox(self.y_train, self.lambda_val)\n",
    "        self.y_test = invboxcox(self.y_test, self.lambda_val)\n",
    "        self.transformations.remove(f\"Box-Cox transformation with lambda {self.lambda_val}\")\n",
    "        self.logger.info(f\"Inverse Box-Cox transformation applied on column {self.target_column}.\")\n",
    "        \n",
    "        if plot:\n",
    "            plt.figure(figsize=(20, 7))\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.title('Inverse Box-Cox Transformed Training Target')\n",
    "            plt.plot(self.y_train, label='Inverse Transformed ' + self.target_column)\n",
    "            plt.legend()\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.title('Inverse Box-Cox Transformed Test Target')\n",
    "            plt.plot(self.y_test, label='Inverse Transformed ' + self.target_column)\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "    def reshape_for_recurrent(self, data: np.array) -> np.array:\n",
    "        self.logger.info(\"Reshaping data for recurrent models...\")\n",
    "        reshaped_data = data.reshape(data.shape)\n",
    "        self.logger.info(f\"Data reshaped to {reshaped_data.shape}.\")\n",
    "        self.transformations.append('Data Reshaped')\n",
    "        return reshaped_data\n",
    "\n",
    "    def generate_sequences(self, X_data: np.array, y_data: np.array, n_steps: int, seq_to_seq: bool = False) -> Tuple[np.array, np.array]:\n",
    "        X, y = [], []\n",
    "        for i in range(len(X_data) - n_steps):\n",
    "            seq_x = X_data[i:i + n_steps, :]\n",
    "            if seq_to_seq:\n",
    "                seq_y = y_data[i:i + n_steps, :]\n",
    "            else:\n",
    "                seq_y = y_data[i + n_steps - 1]\n",
    "            X.append(seq_x)\n",
    "            y.append(seq_y)\n",
    "        self.logger.info(f\"Generated {len(X)} sequences of shape {X[0].shape}.\")\n",
    "        self.transformations.append('Sequences Generated')\n",
    "        return np.array(X), np.array(y)\n",
    "    \n",
    "    def prepare_data_for_recurrent(self, n_steps: int, seq_to_seq: bool = False) -> Tuple[np.array, np.array, np.array, np.array]:\n",
    "        self.logger.info(f\"Preparing data for recurrent models with {n_steps} timesteps...\")\n",
    "        X_train_seq, y_train_seq = self.generate_sequences(self.X_train, self.y_train, n_steps, seq_to_seq)\n",
    "        X_test_seq, y_test_seq = self.generate_sequences(self.X_test, self.y_test, n_steps, seq_to_seq)\n",
    "\n",
    "        # Update instance variables here\n",
    "        self.X_train_seq = self.reshape_for_recurrent(X_train_seq)\n",
    "        self.X_test_seq = self.reshape_for_recurrent(X_test_seq)\n",
    "        self.y_train_seq = y_train_seq  # Assuming y_train_seq and y_test_seq are already 2D\n",
    "        self.y_test_seq = y_test_seq\n",
    "\n",
    "        self.logger.info(\"Data preparation for recurrent models completed.\")\n",
    "        return self.X_train_seq, self.y_train_seq, self.X_test_seq, self.y_test_seq\n",
    "\n",
    "    def prepare_for_prophet(self) -> pd.DataFrame:\n",
    "        prophet_data = self.data[[self.target_column]].reset_index()\n",
    "        prophet_data.columns = ['ds', 'y']\n",
    "        return prophet_data\n",
    "\n",
    "    def get_preprocessed_data(self) -> Tuple[np.array, np.array, np.array, np.array]:\n",
    "        return self.X_train, self.y_train, self.X_test, self.y_test\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return \"Transformations applied: \" + \", \".join(self.transformations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82f498a-9061-415a-9602-b1b857b76504",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# LSTM Sequece-to-One\n",
    "#tsa = TimeSeriesAnalysis(df, target='Close')\n",
    "d_preprocessor = UnifiedDataPreprocessor(df, target_column='Close')\n",
    "d_preprocessor.split_and_plot_data(test_size=0.2,plot=False)\n",
    "d_preprocessor.normalize_data(scaler_type='MinMax',plot=False)\n",
    "d_preprocessor.normalize_target(scaler_type='MinMax',plot=False)\n",
    "n_steps = 10 \n",
    "X_train_seq, y_train_seq, X_test_seq, y_test_seq = d_preprocessor.prepare_data_for_recurrent(n_steps, seq_to_seq=False)\n",
    "str(d_preprocessor)\n",
    "\n",
    "# LSTM Sequece-to-Sequence\n",
    "d_preprocessor = UnifiedDataPreprocessor(df, target_column='Close')\n",
    "d_preprocessor.split_and_plot_data(test_size=0.2, plot=False)\n",
    "d_preprocessor.normalize_data(scaler_type='MinMax', plot=False)\n",
    "d_preprocessor.normalize_target(scaler_type='MinMax', plot=False)\n",
    "n_steps = 10 \n",
    "X_train_seq1, y_train_seq1, X_test_seq1, y_test_seq1 = d_preprocessor.prepare_data_for_recurrent(n_steps, seq_to_seq=True)\n",
    "str(d_preprocessor)\n",
    "\n",
    "# For Linear Regression\n",
    "d_preprocessor = UnifiedDataPreprocessor(df, target_column='Close')\n",
    "d_preprocessor.split_and_plot_data(test_size=0.2, plot=False)\n",
    "d_preprocessor.normalize_data(scaler_type='MinMax', plot=False)\n",
    "d_preprocessor.normalize_target(scaler_type='MinMax', plot=False)\n",
    "X_train_lr, y_train_lr, X_test_lr, y_test_lr = d_preprocessor.get_preprocessed_data()\n",
    "str(d_preprocessor)\n",
    "\n",
    "# For XGBoost \n",
    "d_preprocessor = UnifiedDataPreprocessor(df, target_column='Close')\n",
    "d_preprocessor.split_and_plot_data(test_size=0.2, plot=False)\n",
    "d_preprocessor.normalize_data(scaler_type='MinMax', plot=False)\n",
    "d_preprocessor.normalize_target(scaler_type='MinMax', plot=False)\n",
    "d_preprocessor.get_preprocessed_data()\n",
    "X_train_xgb, y_train_xgb, X_test_xgb, y_test_xgb = d_preprocessor.get_preprocessed_data()\n",
    "str(d_preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e965fe31-8271-483d-a663-c1ba245037cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LSTM Sequence-to-One Data Shapes:\")\n",
    "print(\"X_train_seq:\", X_train_seq.shape)\n",
    "print(\"y_train_seq:\", y_train_seq.shape)\n",
    "print(\"X_test_seq:\", X_test_seq.shape)\n",
    "print(\"y_test_seq:\", y_test_seq.shape)\n",
    "print(\"----\")\n",
    "\n",
    "print(\"LSTM Sequence-to-Sequence Data Shapes:\")\n",
    "print(\"X_train_seq:\", X_train_seq1.shape)\n",
    "print(\"y_train_seq:\", y_train_seq1.shape)\n",
    "print(\"X_test_seq:\", X_test_seq1.shape)\n",
    "print(\"y_test_seq:\", y_test_seq1.shape)\n",
    "print(\"----\")\n",
    "\n",
    "print(\"LR Data Shapes:\")\n",
    "print(\"X_train_lr:\", X_train_lr.shape)\n",
    "print(\"y_train_lr:\", y_train_lr.shape)\n",
    "print(\"X_test_lr:\", X_test_lr.shape)\n",
    "print(\"y_test_lr:\", y_test_lr.shape)\n",
    "print(\"----\")\n",
    "\n",
    "\n",
    "print(\"XGB Data Shapes:\")\n",
    "print(\"X_train_xgb:\", X_train_xgb.shape)\n",
    "print(\"y_train_xgb:\", y_train_xgb.shape)\n",
    "print(\"X_test_xgb:\", X_test_xgb.shape)\n",
    "print(\"y_test_xgb:\", y_test_xgb.shape)\n",
    "print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdb1b65-938f-4f38-a78e-25b3982e2824",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d5c599-0d29-4aee-b20d-b9f78b9ee2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CryptoDataAnalytics:\n",
    "    \"\"\"\n",
    "    This class is responsible for performing enhanced analytics on cryptocurrency data.\n",
    "\n",
    "    Attributes:\n",
    "        df (pd.DataFrame): The DataFrame containing the cryptocurrency data.\n",
    "        output_dir (str): The directory where analytics files will be saved.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, crypto_data: pd.DataFrame):\n",
    "        logger.info(\"Initializing CryptoDataAnalytics class.\")\n",
    "        self.df = crypto_data\n",
    "        self.output_dir = 'analytics_csv'\n",
    "        self._create_output_dir()\n",
    "        logger.info(\"CryptoDataAnalytics class initialized successfully.\")\n",
    "        \n",
    "    def _create_output_dir(self):\n",
    "        \"\"\"Create output directory if it doesn't exist.\"\"\"\n",
    "        if not os.path.exists(self.output_dir):\n",
    "            os.makedirs(self.output_dir)\n",
    "            logger.info(f\"Created output directory: {self.output_dir}\")\n",
    "            \n",
    "    def calculate_historical_volatility(self, column: str = 'Close', window: int = 30) -> pd.DataFrame:\n",
    "        \"\"\"Calculates historical volatility.\"\"\"\n",
    "        logger.info(\"Initiating historical volatility calculation.\")\n",
    "        if len(self.df) < window:\n",
    "            logger.error(\"Data length is less than the rolling window size. Cannot calculate volatility.\")\n",
    "            raise ValueError(\"Insufficient data for volatility calculation.\")\n",
    "        \n",
    "        log_ret = np.log(self.df[column] / self.df[column].shift(1))\n",
    "        volatility = log_ret.rolling(window=window).std()\n",
    "        logger.info(\"Historical volatility calculation successful.\")\n",
    "        return pd.DataFrame(volatility, columns=['Historical Volatility'])\n",
    "        \n",
    "    def perform_time_analysis(self, freq: str):\n",
    "        \"\"\"Performs time-based analysis.\"\"\"\n",
    "        logger.info(f\"Initiating {freq}-based time analysis.\")\n",
    "        data = self.df.resample(freq).agg({'Close': ['last', 'mean', 'max', 'min'], 'Open': 'first'})\n",
    "        data.columns = data.columns.map('_'.join).str.strip('_')\n",
    "        data = self.calculate_price_variation(data)\n",
    "        \n",
    "        # Reorder columns\n",
    "        ordered_columns = ['Close_mean', 'Close_max', 'Close_min', 'Close_last', 'Open_first', 'variation_$_abs', 'variation_%_rel']\n",
    "        data = data[ordered_columns]\n",
    "        \n",
    "        logger.info(f\"{freq}-based time analysis successful.\")\n",
    "        return data\n",
    "\n",
    "    def calculate_price_variation(self, data: pd.DataFrame):\n",
    "        \"\"\"Calculates price variation.\"\"\"\n",
    "        logger.info(\"Initiating price variation calculation.\")\n",
    "        data['variation_$_abs'] = data['Close_last'] - data['Open_first']\n",
    "        data['variation_%_rel'] = ((data['Close_last'] - data['Open_first']) / data['Open_first']) * 100\n",
    "        logger.info(\"Price variation calculation successful.\")\n",
    "        return data\n",
    "    \n",
    "    def retrieve_all_time_records(self):\n",
    "        \"\"\"Retrieves all-time price records.\"\"\"\n",
    "        logger.info(\"Initiating retrieval of all-time records.\")\n",
    "        all_time_high = self.df['Close'].max()\n",
    "        all_time_low = self.df['Close'].min()\n",
    "        all_time_high_date = self.df['Close'].idxmax().strftime('%Y-%m-%d')\n",
    "        all_time_low_date = self.df['Close'].idxmin().strftime('%Y-%m-%d')\n",
    "        logger.info(\"All-time records retrieval successful.\")\n",
    "        return all_time_high, all_time_low, all_time_high_date, all_time_low_date\n",
    "    \n",
    "    def perform_and_save_all_analyses(self):\n",
    "        \"\"\"Performs all analyses and saves them to Excel files.\"\"\"\n",
    "        logger.info(\"Initiating all analyses.\")\n",
    "        self.save_analysis_to_excel(self.perform_time_analysis('Y'), 'yearly_data.xlsx')\n",
    "        self.save_analysis_to_excel(self.perform_time_analysis('M'), 'monthly_data.xlsx')\n",
    "        self.save_analysis_to_excel(self.perform_time_analysis('W'), 'weekly_data.xlsx')\n",
    "        logger.info(\"All analyses have been successfully performed and saved.\")\n",
    "        \n",
    "    def save_analysis_to_excel(self, analysis: pd.DataFrame, filename: str):\n",
    "        \"\"\"Saves the given DataFrame to an Excel file in the output directory.\"\"\"\n",
    "        filepath = os.path.join(self.output_dir, filename)\n",
    "        analysis.to_excel(filepath)\n",
    "        logger.info(f\"Analysis saved to {filepath}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65e323e-c11d-453b-b57e-ed09809d7703",
   "metadata": {},
   "outputs": [],
   "source": [
    "analytics = CryptoDataAnalytics(btc_data)\n",
    "\n",
    "# Retrieve and display all-time records\n",
    "all_time_high, all_time_low, all_time_high_date, all_time_low_date = analytics.retrieve_all_time_records()\n",
    "print(f\"All Time High: {all_time_high} on {all_time_high_date}\")\n",
    "print(f\"All Time Low: {all_time_low} on {all_time_low_date}\")\n",
    "\n",
    "# Run all analyses and save them\n",
    "analytics.perform_and_save_all_analyses()\n",
    "\n",
    "# Additionally, display the DataFrames\n",
    "yearly_data = analytics.perform_time_analysis('Y')\n",
    "monthly_data = analytics.perform_time_analysis('M')\n",
    "weekly_data = analytics.perform_time_analysis('W')\n",
    "\n",
    "display(yearly_data)\n",
    "display(monthly_data)\n",
    "display(weekly_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df16cf96-6644-4d23-8556-1f75b2fe2725",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CryptoAnalyticsVisual:\n",
    "    \"\"\"\n",
    "    The CryptoAnalyticsVisual class provides tools for cryptocurrency market analysis and visualization.\n",
    "    \n",
    "    Attributes:\n",
    "        data (pd.DataFrame): Raw crypto data with 'Open', 'Close', 'High', 'Low', 'Volume'.\n",
    "    \n",
    "    Methods:\n",
    "        _create_visualizations_directory: Creates directory for visualizations.\n",
    "        save_plot_to_file: Saves Bokeh plot to file.\n",
    "        calculate_macd, plot_macd_bokeh: Handles MACD calculation and plotting.\n",
    "        calculate_rsi, plot_rsi_bokeh: Handles RSI calculation and plotting.\n",
    "        calculate_bollinger_bands, plot_bollinger_bands_bokeh: Handles Bollinger Bands.\n",
    "        calculate_fibonacci_retracement, plot_fibonacci_retracement_bokeh: Handles Fibonacci retracement.\n",
    "        volume_analysis, plot_volume_analysis_bokeh: Handles volume analysis.\n",
    "        create_candlestick_chart, plot_trend_bokeh: Plots candlestick and trend data.\n",
    "\n",
    "    Example:\n",
    "        >>> df = pd.read_csv('crypto_data.csv')\n",
    "        >>> analytics = CryptoAnalyticsVisual(df)\n",
    "        >>> analytics.plot_macd_bokeh()\n",
    "    \"\"\"\n",
    "    def __init__(self, data: pd.DataFrame):\n",
    "        self.data = data\n",
    "        output_notebook()\n",
    "        curdoc().theme = 'dark_minimal'\n",
    "        self._create_visualizations_directory()\n",
    "        logger.info('CryptoAnalyticsVisual instance created and initialized.')\n",
    "\n",
    "    def _create_visualizations_directory(self):\n",
    "        \"\"\"Creates a directory for storing visualization assets.\"\"\"\n",
    "        if not os.path.exists('visualizations_assets'):\n",
    "            os.makedirs('visualizations_assets')\n",
    "            logger.info(\"Created directory: visualizations_assets\")\n",
    "\n",
    "    def save_plot_to_file(self, plot, filename: str, format: str = 'html'):\n",
    "        \"\"\"Saves plot to a file.\"\"\"\n",
    "        full_path = os.path.join('visualizations_assets', filename)\n",
    "        if format == 'html':\n",
    "            save(plot, filename=full_path)\n",
    "            logger.info(f'Plot saved to file: {full_path}')\n",
    "        else:\n",
    "            logger.error('Unsupported file format: {}'.format(format))\n",
    "\n",
    "    def calculate_macd(self, short_window=12, long_window=26, signal_window=9):\n",
    "        \"\"\"Calculates MACD and signal line.\"\"\"\n",
    "        short_ema = self.data['Close'].ewm(span=short_window, adjust=False).mean()\n",
    "        long_ema = self.data['Close'].ewm(span=long_window, adjust=False).mean()\n",
    "        macd_line = short_ema - long_ema\n",
    "        signal_line = macd_line.ewm(span=signal_window, adjust=False).mean()\n",
    "        logger.info(f'MACD calculated with short_window={short_window}, long_window={long_window}, and signal_window={signal_window}')\n",
    "        return macd_line, signal_line\n",
    "\n",
    "    def plot_macd_bokeh(self, display=True):\n",
    "        \"\"\"Plots MACD and signal line.\"\"\"\n",
    "        macd_line, signal_line = self.calculate_macd()\n",
    "        source = ColumnDataSource(data=dict(x=self.data.index, y1=macd_line, y2=signal_line))\n",
    "        p = figure(width=1400, height=600, title=\"MACD Analysis\", x_axis_type=\"datetime\")\n",
    "        p.line(x='x', y='y1', source=source, legend_label=\"MACD Line\", color=\"blue\", alpha=0.8)\n",
    "        p.line(x='x', y='y2', source=source, legend_label=\"Signal Line\", color=\"red\", alpha=0.8)\n",
    "        hover = HoverTool(tooltips=[(\"Date\", \"@x{%F}\"), (\"MACD\", \"@y1\"), (\"Signal\", \"@y2\")], formatters={\"@x\": \"datetime\"})\n",
    "        p.add_tools(hover)\n",
    "        logger.info('MACD plot displayed.')\n",
    "        if display:\n",
    "            show(p)\n",
    "        return p\n",
    "\n",
    "    def calculate_rsi(self, window=14):\n",
    "        \"\"\"Calculates the Relative Strength Index (RSI).\"\"\"\n",
    "        delta = self.data['Close'].diff()\n",
    "        gain = (delta.where(delta > 0, 0)).fillna(0)\n",
    "        loss = (-delta.where(delta < 0, 0)).fillna(0)\n",
    "        avg_gain = gain.rolling(window=window, min_periods=1).mean()\n",
    "        avg_loss = loss.rolling(window=window, min_periods=1).mean()\n",
    "        rs = avg_gain / avg_loss\n",
    "        rsi = 100 - (100 / (1 + rs))\n",
    "        logger.info(f'RSI calculated with window={window}')\n",
    "        return rsi\n",
    "\n",
    "    def plot_rsi_bokeh(self, display=True):\n",
    "        \"\"\"Plots the Relative Strength Index (RSI).\"\"\"\n",
    "        rsi = self.calculate_rsi()\n",
    "        source = ColumnDataSource(data=dict(x=self.data.index, y=rsi))\n",
    "        p = figure(width=1400, height=600, title=\"RSI Analysis\", x_axis_type=\"datetime\")\n",
    "        p.line(x='x', y='y', source=source, legend_label=\"RSI\", color=\"green\", alpha=0.8)\n",
    "        hover = HoverTool(tooltips=[(\"Date\", \"@x{%F}\"), (\"RSI\", \"@y\")], formatters={\"@x\": \"datetime\"})\n",
    "        p.add_tools(hover)\n",
    "        p.add_layout(Span(location=70, dimension='width', line_color='red', line_width=1, line_dash='dashed'))\n",
    "        p.add_layout(Span(location=30, dimension='width', line_color='red', line_width=1, line_dash='dashed'))\n",
    "        logger.info('RSI plot displayed.')\n",
    "        if display:\n",
    "            show(p)\n",
    "        return p\n",
    "\n",
    "    def calculate_bollinger_bands(self, window=20, num_std=2):\n",
    "        \"\"\"Calculates upper and lower Bollinger Bands.\"\"\"\n",
    "        rolling_mean = self.data['Close'].rolling(window=window).mean()\n",
    "        rolling_std = self.data['Close'].rolling(window=window).std()\n",
    "        upper_band = rolling_mean + (rolling_std * num_std)\n",
    "        lower_band = rolling_mean - (rolling_std * num_std)\n",
    "        logger.info(f'Bollinger Bands calculated with window={window} and num_std={num_std}')\n",
    "        return upper_band, lower_band\n",
    "\n",
    "    def plot_bollinger_bands_bokeh(self, display=True):\n",
    "        \"\"\"Plots Bollinger Bands and Close Price.\"\"\"\n",
    "        upper_band, lower_band = self.calculate_bollinger_bands()\n",
    "        source = ColumnDataSource(data=dict(x=self.data.index, close=self.data['Close'], upper=upper_band, lower=lower_band))\n",
    "        p = figure(width=1400, height=600, title=\"Bollinger Bands Analysis\", x_axis_type=\"datetime\")\n",
    "        p.line(x='x', y='close', source=source, legend_label=\"Close Price\", color=\"blue\", alpha=0.8)\n",
    "        p.line(x='x', y='upper', source=source, legend_label=\"Upper Band\", color=\"red\", alpha=0.5)\n",
    "        p.line(x='x', y='lower', source=source, legend_label=\"Lower Band\", color=\"green\", alpha=0.5)\n",
    "        hover = HoverTool(tooltips=[(\"Date\", \"@x{%F}\"), (\"Close\", \"@close{$0,0.00} K\"), (\"Upper Band\", \"@upper{$0,0.00} K\"), (\"Lower Band\", \"@lower{$0,0.00} K\")], formatters={\"@x\": \"datetime\"})\n",
    "        p.yaxis.formatter = NumeralTickFormatter(format=\"$0,0.00\")\n",
    "        p.add_tools(hover)\n",
    "        logger.info('Bollinger Bands plot displayed.')\n",
    "        if display:\n",
    "            show(p)\n",
    "        return p\n",
    "\n",
    "    def calculate_fibonacci_retracement(self):\n",
    "        \"\"\"Calculates Fibonacci retracement levels.\"\"\"\n",
    "        max_price = self.data['High'].max()\n",
    "        min_price = self.data['Low'].min()\n",
    "        diff = max_price - min_price\n",
    "        levels = [0.0, 0.236, 0.382, 0.5, 0.618, 0.786, 1.0]\n",
    "        retracement_levels = {level: (max_price - level * diff) for level in levels}\n",
    "        logger.info('Fibonacci retracement levels calculated.')\n",
    "        return retracement_levels\n",
    "\n",
    "    def plot_fibonacci_retracement_bokeh(self, display=True):\n",
    "        \"\"\"Plots Fibonacci retracement levels.\"\"\"\n",
    "        retracement_levels = self.calculate_fibonacci_retracement()\n",
    "        source = ColumnDataSource(data=dict(x=self.data.index, close=self.data['Close']))\n",
    "        p = figure(width=1400, height=600, title=\"Fibonacci Retracement Levels\", x_axis_type=\"datetime\")\n",
    "        p.line(x='x', y='close', source=source, legend_label=\"Close Price\", color=\"blue\", alpha=0.8)\n",
    "        for level, price in retracement_levels.items():\n",
    "            p.add_layout(Span(location=price, dimension='width', line_color='red', line_width=1, line_dash='dashed'))\n",
    "            p.line([], [], line_color=\"red\", legend_label=f'Level: {level}', line_dash='dashed')\n",
    "        hover = HoverTool(tooltips=[(\"Date\", \"@x{%F}\"), (\"Close Price\", \"@close{$0,0.00} K\")], formatters={\"@x\": \"datetime\"})\n",
    "        p.yaxis.formatter = NumeralTickFormatter(format=\"$0,0.00\")\n",
    "        p.add_tools(hover)\n",
    "        logger.info('Fibonacci retracement plot displayed.')\n",
    "        if display:\n",
    "            show(p)\n",
    "        return p\n",
    "\n",
    "    def volume_analysis(self):\n",
    "        \"\"\"Analyzes volume data and computes average volume over 30 days.\"\"\"\n",
    "        volume = self.data['Volume'] / 1_000  # Convert to Thousands\n",
    "        avg_volume = volume.rolling(window=30).mean()\n",
    "        logger.info('Volume analysis completed.')\n",
    "        return volume, avg_volume\n",
    "\n",
    "    def plot_volume_analysis_bokeh(self, display=True):\n",
    "        \"\"\"Plots volume and 30-day average volume.\"\"\"\n",
    "        volume, avg_volume = self.volume_analysis()\n",
    "        source = ColumnDataSource(data=dict(x=self.data.index, volume=volume, avg_volume=avg_volume))\n",
    "        p = figure(width=1400, height=600, title=\"Volume Analysis (in Thousands)\", x_axis_type=\"datetime\")\n",
    "        p.vbar(x='x', top='volume', source=source, width=0.9, legend_label=\"Volume\", alpha=0.6, color=\"blue\")\n",
    "        p.line(x='x', y='avg_volume', source=source, legend_label=\"30-Day Avg Volume\", color=\"red\", line_width=2)\n",
    "        hover = HoverTool(tooltips=[(\"Date\", \"@x{%F}\"), (\"Volume\", \"@volume{$0,0} K\"), (\"30-Day Avg Volume\", \"@avg_volume{$0,0} K\")], formatters={\"@x\": \"datetime\"})\n",
    "        p.yaxis.formatter = NumeralTickFormatter(format=\"$0,0\")\n",
    "        p.add_tools(hover)\n",
    "        logger.info('Volume analysis plot displayed.')\n",
    "        if display:\n",
    "            show(p) \n",
    "        return p\n",
    "    \n",
    "    def create_candlestick_chart(self, time_period='last_month', ma_period=20, display=True):\n",
    "        \"\"\"\n",
    "        Creates a candlestick chart for the selected time period and moving average period.\n",
    "        \"\"\"\n",
    "        logger.info(\"Creating candlestick chart.\")\n",
    "        # Assuming _select_data is a method that filters the data based on the time_period\n",
    "        df = self._select_data(time_period)\n",
    "\n",
    "        df['index_col'] = df.index  \n",
    "        df['MA'] = df['Close'].rolling(window=ma_period).mean()\n",
    "\n",
    "        inc = df.Close > df.Open\n",
    "        dec = df.Open > df.Close\n",
    "\n",
    "        source_inc = ColumnDataSource(df[inc])\n",
    "        source_dec = ColumnDataSource(df[dec])\n",
    "        source_hover = ColumnDataSource(df)\n",
    "        w = 12 * 60 * 60 * 1000 \n",
    "\n",
    "        TOOLS = \"pan,wheel_zoom,box_zoom,reset,save\"\n",
    "        p = figure(x_axis_type=\"datetime\", tools=TOOLS, width=1400, title=\"Crypto Candlestick with MA\")\n",
    "        p.xaxis.major_label_orientation = pi / 4\n",
    "        p.grid.grid_line_alpha = 0.3\n",
    "\n",
    "        p.segment('index_col', 'High', 'index_col', 'Low', color=\"black\", source=source_inc)\n",
    "        p.vbar('index_col', w, 'Open', 'Close', fill_color=\"#39B86B\", line_color=\"black\", source=source_inc)\n",
    "        p.segment('index_col', 'High', 'index_col', 'Low', color=\"black\", source=source_dec)\n",
    "        p.vbar('index_col', w, 'Open', 'Close', fill_color=\"#F2583E\", line_color=\"black\", source=source_dec)\n",
    "\n",
    "        hover = HoverTool(\n",
    "            tooltips=[\n",
    "                (\"Date\", \"@index_col{%F}\"),\n",
    "                (\"Open\", \"@{Open}{($ 0,0.00)}\"),\n",
    "                (\"Close\", \"@{Close}{($ 0,0.00)}\"),\n",
    "                (\"High\", \"@{High}{($ 0,0.00)}\"),\n",
    "                (\"Low\", \"@{Low}{($ 0,0.00)}\"),\n",
    "                (\"MA\", \"@{MA}{($ 0,0.00)}\")\n",
    "            ],\n",
    "            formatters={\n",
    "                '@index_col': 'datetime',\n",
    "                '@Open': 'numeral',\n",
    "                '@Close': 'numeral',\n",
    "                '@High': 'numeral',\n",
    "                '@Low': 'numeral',\n",
    "                '@MA': 'numeral'\n",
    "            },\n",
    "            mode='vline'\n",
    "        )\n",
    "        p.add_tools(hover)\n",
    "        p.line('index_col', 'MA', color='blue', legend_label='Moving Average', source=source_hover)\n",
    "\n",
    "        if display:\n",
    "            show(p)\n",
    "\n",
    "        logger.info('Candlestick chart displayed.')\n",
    "        return p\n",
    "\n",
    "    def plot_trend_bokeh(self, display=True):\n",
    "        \"\"\"\n",
    "        Plots trend data using various moving averages for analysis.\n",
    "        \"\"\"\n",
    "        logger.info(\"Creating trend analysis plot.\")\n",
    "        # Assuming _identify_trend is a method that identifies the trend in the data\n",
    "        trend_data = self._identify_trend()\n",
    "        source = ColumnDataSource(data={**{'x': self.data.index, 'price': trend_data['Price']}, **{f\"mavg{period}\": trend_data[f\"{period}_day_mavg\"] for period in [3, 7, 15, 40, 90, 120]}})\n",
    "\n",
    "        p = figure(width=1400, height=600, title=\"Trend Analysis using Moving Averages\", x_axis_type=\"datetime\")\n",
    "        p.line(x='x', y='price', source=source, legend_label=\"Close Price\", alpha=0.8)\n",
    "\n",
    "        colors = {\"3\": \"orange\", \"7\": \"yellow\", \"15\": \"cyan\", \"40\": \"red\", \"90\": \"purple\", \"120\": \"green\"}\n",
    "        for period, color in colors.items():\n",
    "            p.line(x='x', y=f'mavg{period}', source=source, legend_label=f\"{period}-day MA\", color=color, line_dash=\"dashed\")\n",
    "\n",
    "        hover = HoverTool(\n",
    "            tooltips=[\n",
    "                (\"Date\", \"@x{%F}\"),\n",
    "                (\"Price\", \"@price{$0,0.00} K\"),\n",
    "                (\"3-day MA\", \"@mavg3{$0,0.00} K\"),\n",
    "                (\"7-day MA\", \"@mavg7{$0,0.00} K\"),\n",
    "                (\"15-day MA\", \"@mavg15{$0,0.00} K\"),\n",
    "                (\"40-day MA\", \"@mavg40{$0,0.00} K\"),\n",
    "                (\"90-day MA\", \"@mavg90{$0,0.00} K\"),\n",
    "                (\"120-day MA\", \"@mavg120{$0,0.00} K\")\n",
    "            ],\n",
    "            formatters={\"@x\": \"datetime\"}\n",
    "        )\n",
    "        p.yaxis.formatter = NumeralTickFormatter(format=\"$0,0.00\")\n",
    "        p.add_tools(hover)\n",
    "        if display:\n",
    "            show(p)\n",
    "\n",
    "        logger.info('Trend plot displayed.')\n",
    "        return p\n",
    "    \n",
    "    def _identify_trend(self, column: str = 'Close'):\n",
    "        signals = pd.DataFrame(index=self.data.index)\n",
    "        signals['Price'] = self.data[column]\n",
    "\n",
    "        # Moving Averages\n",
    "        ma_periods = [3, 7, 15, 40, 90, 120]\n",
    "        for period in ma_periods:\n",
    "            signals[f'{period}_day_mavg'] = self.data[column].rolling(window=period, min_periods=1, center=False).mean()\n",
    "\n",
    "        # Signal based on 40-day and 120-day moving averages (since there's no 100-day moving average in the new setup)\n",
    "        signals['signal'] = 0.0\n",
    "        signals['signal'][40:] = np.where(signals['40_day_mavg'][40:] > signals['120_day_mavg'][40:], 1.0, 0.0)\n",
    "        return signals\n",
    "\n",
    "    def _select_data(self, time_period):\n",
    "        logger.info(\"Selecting data for time period: %s\", time_period)\n",
    "        if time_period == 'last_month':\n",
    "            last_month = self.data.index.max() - pd.DateOffset(months=1)\n",
    "            df = self.data[self.data.index >= last_month]\n",
    "        elif time_period == 'last_3_months':\n",
    "            last_3_months = self.data.index.max() - pd.DateOffset(months=3)\n",
    "            df = self.data[self.data.index >= last_3_months]\n",
    "        elif time_period == 'last_6_months':\n",
    "            last_6_months = self.data.index.max() - pd.DateOffset(months=6)\n",
    "            df = self.data[self.data.index >= last_6_months]\n",
    "        elif time_period == 'last_1_year':\n",
    "            last_1_year = self.data.index.max() - pd.DateOffset(years=1)\n",
    "            df = self.data[self.data.index >= last_1_year]\n",
    "        elif time_period == 'last_3_years':\n",
    "            last_3_years = self.data.index.max() - pd.DateOffset(years=3)\n",
    "            df = self.data[self.data.index >= last_3_years]\n",
    "        else:\n",
    "            df = self.data\n",
    "\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07627dda-7eec-4cf0-8bc0-f5aae191fd73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "crypto_analytics = CryptoAnalyticsVisual(btc_data)\n",
    "candle = crypto_analytics.create_candlestick_chart(time_period='last_6_months', ma_period=20)\n",
    "trend = crypto_analytics.plot_trend_bokeh()\n",
    "bollinger_bands = crypto_analytics.plot_bollinger_bands_bokeh()\n",
    "macd = crypto_analytics.plot_macd_bokeh()\n",
    "rsi = crypto_analytics.plot_rsi_bokeh()\n",
    "fibonacci_retracement = crypto_analytics.plot_fibonacci_retracement_bokeh()\n",
    "volume = crypto_analytics.plot_volume_analysis_bokeh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02163058-95f3-4227-932e-7bf096250d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "crypto_analytics.save_plot_to_file(candle, 'candle.html')\n",
    "crypto_analytics.save_plot_to_file(trend, 'trend.html')\n",
    "crypto_analytics.save_plot_to_file(bollinger_bands, 'bollinger_bands.html')\n",
    "crypto_analytics.save_plot_to_file(macd, 'macd.html')\n",
    "crypto_analytics.save_plot_to_file(rsi, 'rsi.html')\n",
    "crypto_analytics.save_plot_to_file(fibonacci_retracement, 'fibonacci_retracement.html')\n",
    "crypto_analytics.save_plot_to_file(volume, 'rsi.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d45c8a-e30e-469e-a0a1-238924ec7bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature_Eng_Tech:\n",
    "    \"\"\"\n",
    "    The Feature_Eng_Tech class is responsible for applying various feature engineering techniques on time series data.\n",
    "    \n",
    "    Attributes:\n",
    "        df (pd.DataFrame): Original time series data.\n",
    "        target_column (str): Target column for which features are being generated.\n",
    "        data_eng (pd.DataFrame): DataFrame with engineered features.\n",
    "        logger (logging.Logger): Logger for tracking operations and debugging.\n",
    "        \n",
    "    Methods:\n",
    "        reset_data: Resets the engineered data to its original state.\n",
    "        handle_missing_values: Handles missing values in the DataFrame.\n",
    "        add_date_features: Adds date-related features like year, month, day, and optionally day of the week.\n",
    "        add_lag_features: Adds lag features based on a given window size.\n",
    "        add_rolling_features: Adds rolling window features like mean and standard deviation.\n",
    "        add_expanding_window_features: Adds expanding window features like mean, min, max, and sum.\n",
    "        add_seasonal_decomposition: Adds seasonal decomposition features like trend, seasonality, and residuals.\n",
    "        detrend_data: Detrends the time series data.\n",
    "        add_holiday_features: Adds a feature to indicate holidays.\n",
    "        add_fourier_features: Adds Fourier features based on a given period and order.\n",
    "        handle_nan_values_post_engineering: Handles NaN values post feature engineering.\n",
    "        feature_engineering: Applies multiple feature engineering methods based on a configuration dictionary.\n",
    "        get_engineered_data: Returns the DataFrame with engineered features.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame, target_column: str):\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            raise ValueError(\"Input data should be a pandas DataFrame.\")\n",
    "        if target_column not in df.columns:\n",
    "            raise ValueError(f\"Target column {target_column} not found in DataFrame.\")\n",
    "        \n",
    "        self.df = df.copy()\n",
    "        self.target_column = target_column\n",
    "        self.data_eng = self.df.copy()\n",
    "        \n",
    "        logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] - %(message)s')\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.logger.info(\"Initialized Feature_Eng_Tech.\")\n",
    "\n",
    "    def reset_data(self):\n",
    "        \"\"\"Resets the engineered data to its original state.\"\"\"\n",
    "        self.data_eng = self.df.copy()\n",
    "        self.logger.info(\"Reset data to the original state.\")\n",
    "\n",
    "    def handle_missing_values(self, method: str = 'ffill'):\n",
    "        \"\"\"\n",
    "        Handles missing values in the DataFrame.\n",
    "        \n",
    "        Parameters:\n",
    "            method (str): Method to handle missing values ('ffill', 'bfill', 'interpolate', 'drop'). Default is 'ffill'.\n",
    "        \"\"\"\n",
    "        if method not in ['ffill', 'bfill', 'interpolate', 'drop']:\n",
    "            raise ValueError(\"Invalid method for handling missing values. Choose 'ffill', 'bfill', 'interpolate', or 'drop'.\")\n",
    "        if self.df.isnull().sum().sum() > 0:\n",
    "            self.df.fillna(method=method, inplace=True)\n",
    "            self.logger.info(f\"Handled missing values using {method} method.\")\n",
    "        else:\n",
    "            self.logger.info(\"No missing values detected.\")\n",
    "\n",
    "    def add_date_features(self, include_day_of_week: bool = True):\n",
    "        \"\"\"\n",
    "        Adds date-related features like year, month, day, and optionally day of the week.\n",
    "        \n",
    "        Parameters:\n",
    "            include_day_of_week (bool): Whether to include the day of the week as a feature. Default is True.\n",
    "        \"\"\"\n",
    "        if not isinstance(self.df.index, pd.DatetimeIndex):\n",
    "            self.df.index = pd.to_datetime(self.df.index)\n",
    "        self.df['year'] = self.df.index.year\n",
    "        self.df['month'] = self.df.index.month\n",
    "        self.df['day'] = self.df.index.day\n",
    "        if include_day_of_week:\n",
    "            self.df['day_of_week'] = self.df.index.dayofweek\n",
    "        self.logger.info(\"Date-related features added.\")\n",
    "\n",
    "    def add_lag_features(self, window: int = 3):\n",
    "        \"\"\"\n",
    "        Adds lag features based on a given window size.\n",
    "        \n",
    "        Parameters:\n",
    "            window (int): The window size for creating lag features. Default is 3.\n",
    "        \"\"\"\n",
    "        if window > len(self.data_eng):\n",
    "            raise ValueError(\"The window parameter should be less than the length of the time series data.\")\n",
    "        for i in range(1, window + 1):\n",
    "            self.data_eng[f\"lag_{i}\"] = self.data_eng[self.target_column].shift(i)\n",
    "        self.logger.info(f'Added lag features with window size {window}.')\n",
    "\n",
    "    def add_rolling_features(self, window: int = 3, min_periods: int = 1):\n",
    "        \"\"\"\n",
    "        Adds rolling window features like mean and standard deviation.\n",
    "        \n",
    "        Parameters:\n",
    "            window (int): The window size for rolling features. Default is 3.\n",
    "            min_periods (int): Minimum number of observations required to have a value. Default is 1.\n",
    "        \"\"\"\n",
    "        self.data_eng[f\"rolling_mean_{window}\"] = self.data_eng[self.target_column].rolling(window=window, min_periods=min_periods).mean()\n",
    "        self.data_eng[f\"rolling_std_{window}\"] = self.data_eng[self.target_column].rolling(window=window, min_periods=min_periods).std()\n",
    "        self.logger.info(f'Added rolling window features with window size {window}.')\n",
    "\n",
    "    def add_expanding_window_features(self, min_periods: int = 1):\n",
    "        \"\"\"\n",
    "        Adds expanding window features like mean, min, max, and sum.\n",
    "        \n",
    "        Parameters:\n",
    "            min_periods (int): Minimum number of observations required to have a value. Default is 1.\n",
    "        \"\"\"\n",
    "        self.data_eng['expanding_mean'] = self.data_eng[self.target_column].expanding(min_periods=min_periods).mean()\n",
    "        self.data_eng['expanding_min'] = self.data_eng[self.target_column].expanding(min_periods=min_periods).min()\n",
    "        self.data_eng['expanding_max'] = self.data_eng[self.target_column].expanding(min_periods=min_periods).max()\n",
    "        self.data_eng['expanding_sum'] = self.data_eng[self.target_column].expanding(min_periods=min_periods).sum()\n",
    "        self.logger.info('Added expanding window features.')\n",
    "\n",
    "    def add_seasonal_decomposition(self, period: int = 12, model: str = 'additive'):\n",
    "        \"\"\"\n",
    "        Adds seasonal decomposition features like trend, seasonality, and residuals.\n",
    "        \n",
    "        Parameters:\n",
    "            period (int): The period for seasonal decomposition. Default is 12.\n",
    "            model (str): The model type for seasonal decomposition ('additive' or 'multiplicative'). Default is 'additive'.\n",
    "        \"\"\"\n",
    "        result = seasonal_decompose(self.data_eng[self.target_column], period=period, model=model)\n",
    "        self.data_eng['trend'] = result.trend\n",
    "        self.data_eng['seasonal'] = result.seasonal\n",
    "        self.data_eng['residual'] = result.resid\n",
    "        self.logger.info(f'Added seasonal decomposition with period {period} and model {model}.')\n",
    "\n",
    "    def detrend_data(self):\n",
    "        \"\"\"Detrends the time series data.\"\"\"\n",
    "        self.data_eng['detrended'] = detrend(self.data_eng[self.target_column])\n",
    "        self.logger.info('Detrended the data.')\n",
    "\n",
    "    def add_holiday_features(self):\n",
    "        \"\"\"Adds a feature to indicate holidays.\"\"\"\n",
    "        cal = USFederalHolidayCalendar()\n",
    "        holidays = cal.holidays(start=self.data_eng.index.min(), end=self.data_eng.index.max())\n",
    "        self.data_eng['is_holiday'] = self.data_eng.index.isin(holidays).astype(int)\n",
    "        self.logger.info('Added holiday features.')\n",
    "\n",
    "    def add_fourier_features(self, period: int, order: int):\n",
    "        \"\"\"\n",
    "        Adds Fourier features based on a given period and order.\n",
    "        \n",
    "        Parameters:\n",
    "            period (int): The period for Fourier features.\n",
    "            order (int): The order for Fourier features.\n",
    "        \"\"\"\n",
    "        for i in range(1, order + 1):\n",
    "            self.data_eng[f'fourier_sin_{i}'] = np.sin(2 * i * np.pi * self.data_eng.index.dayofyear / period)\n",
    "            self.data_eng[f'fourier_cos_{i}'] = np.cos(2 * i * np.pi * self.data_eng.index.dayofyear / period)\n",
    "        self.logger.info(f'Added Fourier features with period {period} and order {order}.')\n",
    "\n",
    "    def handle_nan_values_post_engineering(self, method: str = 'drop'):\n",
    "        \"\"\"\n",
    "        Handles NaN values post feature engineering.\n",
    "        \n",
    "        Parameters:\n",
    "            method (str): Method to handle missing values ('drop', 'ffill', 'bfill'). Default is 'drop'.\n",
    "        \"\"\"\n",
    "        if method == 'drop':\n",
    "            self.data_eng.dropna(inplace=True)\n",
    "        elif method == 'ffill':\n",
    "            self.data_eng.fillna(method='ffill', inplace=True)\n",
    "        elif method == 'bfill':\n",
    "            self.data_eng.fillna(method='bfill', inplace=True)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid method. Choose 'drop', 'ffill', or 'bfill'.\")\n",
    "        self.logger.info(f\"Handled NaN values using {method} method.\")\n",
    "\n",
    "    def feature_engineering(self, config: dict):\n",
    "        \"\"\"\n",
    "        Applies multiple feature engineering methods based on a configuration dictionary.\n",
    "        \n",
    "        Parameters:\n",
    "            config (dict): A dictionary with the configuration for feature engineering.\n",
    "        \"\"\"\n",
    "        feature_methods = {\n",
    "            \"handle_missing_values\": self.handle_missing_values,\n",
    "            \"add_date_features\": self.add_date_features,\n",
    "            \"add_lag_features\": self.add_lag_features,\n",
    "            \"add_rolling_features\": self.add_rolling_features,\n",
    "            \"add_expanding_window_features\": self.add_expanding_window_features,\n",
    "            \"add_seasonal_decomposition\": self.add_seasonal_decomposition,\n",
    "            \"detrend_data\": self.detrend_data,\n",
    "            \"add_holiday_features\": self.add_holiday_features,\n",
    "            \"add_fourier_features\": lambda: self.add_fourier_features(config.get(\"fourier_period\", 365), config.get(\"fourier_order\", 3))\n",
    "        }\n",
    "\n",
    "        for feature, method in feature_methods.items():\n",
    "            if config.get(feature):\n",
    "                method()\n",
    "        self.handle_nan_values_post_engineering()\n",
    "        self.logger.info('Feature engineering steps applied based on configuration.')\n",
    "\n",
    "    def get_engineered_data(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Returns the DataFrame with engineered features.\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame containing the engineered features.\n",
    "        \"\"\"\n",
    "        return self.data_eng.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e20cae-0c1d-4774-b446-04221264c39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_eng = Feature_Eng_Tech(btc_data, target_column='Close')\n",
    "\n",
    "# Define a configuration for feature engineering\n",
    "config = {\n",
    "    \"handle_missing_values\": True,\n",
    "    \"add_date_features\": True,\n",
    "    \"add_lag_features\": True,\n",
    "    \"add_rolling_features\": True,\n",
    "    \"add_expanding_window_features\": True,\n",
    "    \"add_seasonal_decomposition\": True,\n",
    "    \"detrend_data\": True,\n",
    "    \"add_holiday_features\": True,\n",
    "    \"add_fourier_features\": True,\n",
    "}\n",
    "\n",
    "# Apply feature engineering based on the configuration\n",
    "feature_eng.feature_engineering(config)\n",
    "\n",
    "# Get the engineered data\n",
    "data_eng = feature_eng.get_engineered_data()\n",
    "data_eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d703272e-96df-4d9c-9aff-d0e1cb650059",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8662a383-b3eb-476a-994b-00e9e8c0e036",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a6ad50-7809-4372-abea-1aa6860b7eb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40cdd6b-4d64-4fa6-9dfb-c1c938867db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesAnalysis1:\n",
    "    \"\"\"\n",
    "    A class to perform various time series analysis tasks such as stationarity checks, volatility modeling, and decomposition.\n",
    "\n",
    "    Attributes:\n",
    "        data (pd.DataFrame): Time series data.\n",
    "        target (str): Target column for time series analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, target):\n",
    "        \"\"\"\n",
    "        Initialize the TimeSeriesAnalysis class.\n",
    "\n",
    "        Parameters:\n",
    "            data (pd.DataFrame): Time series data.\n",
    "            target (str): Target column for time series analysis.\n",
    "        \"\"\"\n",
    "        if target not in data.columns:\n",
    "            raise ValueError(f\"'{target}' is not a column in the provided data.\")\n",
    "        self.original_data = data.copy()\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self.alpha = 0.05  \n",
    "\n",
    "    def check_autocorrelation(self, show_plot=True):\n",
    "        \"\"\"\n",
    "        Check the autocorrelation of the time series using ACF and PACF plots.\n",
    "\n",
    "        Returns:\n",
    "            tuple: ACF and PACF figures.\n",
    "        \"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1,2, figsize=(20,6))\n",
    "        plot_acf(self.data[self.target], lags=50, alpha=0.05, ax=ax1)\n",
    "        ax1.set_title(\"ACF for {}\".format(self.target))\n",
    "        plot_pacf(self.data[self.target], lags=50, alpha=0.05, method='ols', ax=ax2)\n",
    "        ax2.set_title(\"PACF for {}\".format(self.target))\n",
    "        self.save_and_show_plot(fig, 'autocorrelation.png', show=show_plot)\n",
    "        return fig\n",
    "    \n",
    "    def decompose_time_series(self, model='additive', period=None, show=True):\n",
    "        \"\"\"\n",
    "        Decompose the time series data into trend, seasonal, and residual components.\n",
    "    \n",
    "        Parameters:\n",
    "            model (str): The type of decomposition model ('additive' or 'multiplicative').\n",
    "            period (int): The period for seasonal decomposition. If None, it will be inferred.\n",
    "            show (bool): Whether to display the plot.\n",
    "        Returns:\n",
    "            dict: A dictionary containing decomposition components and guidance.\n",
    "        \"\"\"\n",
    "        logger.info(\"Decomposing the time series\")\n",
    "        if period is None:\n",
    "            # Attempt to infer the seasonal period\n",
    "            period = self.infer_seasonal_period()\n",
    "            \n",
    "        result = seasonal_decompose(self.data[self.target], model=model, period=period)\n",
    "    \n",
    "        # Adjusting the figsize here\n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 12))\n",
    "    \n",
    "        result.observed.plot(ax=ax1)\n",
    "        ax1.set_title('Observed')\n",
    "        result.trend.plot(ax=ax2)\n",
    "        ax2.set_title('Trend')\n",
    "        result.seasonal.plot(ax=ax3)\n",
    "        ax3.set_title('Seasonal')\n",
    "        result.resid.plot(ax=ax4)\n",
    "        ax4.set_title('Residual')\n",
    "    \n",
    "        self.save_and_show_plot(fig, 'decompose.png', show=show)\n",
    "        \n",
    "        # Analyzing the decomposition results\n",
    "        guidance = self.analyze_decomposition(result)\n",
    "        \n",
    "        # Returning the decomposition components and guidance\n",
    "        return {\n",
    "            'Observed': result.observed,\n",
    "            'Trend': result.trend,\n",
    "            'Seasonal': result.seasonal,\n",
    "            'Residual': result.resid,\n",
    "            'Guidance': guidance\n",
    "        }\n",
    "\n",
    "    def analyze_decomposition(self, decomposition_result):\n",
    "        \"\"\"\n",
    "        Analyze the time series decomposition results and provide guidance.\n",
    "    \n",
    "        Parameters:\n",
    "            decomposition_result (DecomposeResult): The result from seasonal decomposition.\n",
    "    \n",
    "        Returns:\n",
    "            str: Guidance based on the decomposition results.\n",
    "        \"\"\"\n",
    "        guidance = \"\"\n",
    "        \n",
    "        # Analyzing the trend component\n",
    "        if decomposition_result.trend.isnull().sum() > 0:\n",
    "            guidance += \"The trend component has missing values at the boundaries. Consider using a different decomposition method or filling the missing values.\\n\"\n",
    "        else:\n",
    "            trend_strength = np.nanmean(np.abs(decomposition_result.trend - np.nanmean(decomposition_result.trend)))\n",
    "            if trend_strength > 0.05 * np.nanmean(decomposition_result.observed):\n",
    "                guidance += \"The trend component is strong. Consider detrending the series if you intend to use models that assume stationarity.\\n\"\n",
    "            else:\n",
    "                guidance += \"The trend component is weak, indicating a relatively stable mean over time.\\n\"\n",
    "                \n",
    "        # Analyzing the seasonal component\n",
    "        seasonal_strength = np.nanmean(np.abs(decomposition_result.seasonal - np.nanmean(decomposition_result.seasonal)))\n",
    "        if seasonal_strength > 0.05 * np.nanmean(decomposition_result.observed):\n",
    "            guidance += \"The seasonal component is strong. Consider seasonal adjustment or using models that can handle seasonality.\\n\"\n",
    "        else:\n",
    "            guidance += \"The seasonal component is weak, indicating that seasonality may not be a significant factor.\\n\"\n",
    "            \n",
    "        # Analyzing the residual component\n",
    "        if decomposition_result.resid.isnull().sum() > 0:\n",
    "            guidance += \"The residual component has missing values at the boundaries. Consider using a different decomposition method or filling the missing values.\\n\"\n",
    "        else:\n",
    "            if np.nanstd(decomposition_result.resid) > 0.05 * np.nanmean(decomposition_result.observed):\n",
    "                guidance += \"The residual component shows variability. Consider further analysis to identify any remaining patterns or anomalies.\\n\"\n",
    "            else:\n",
    "                guidance += \"The residual component is relatively stable, indicating that most of the patterns have been captured by the trend and seasonal components.\\n\"\n",
    "                \n",
    "        return guidance\n",
    "        \n",
    "    def save_and_show_plot(self, fig, filename, show=True):\n",
    "        \"\"\"\n",
    "        Utility method to save and display the plot.\n",
    "\n",
    "        Parameters:\n",
    "            fig (matplotlib.figure.Figure): The plot figure.\n",
    "            filename (str): Filename to save the plot.\n",
    "            show (bool, optional): Whether to display the plot. Default is True.\n",
    "        \"\"\"\n",
    "        if not os.path.exists('ts_plots_assets'):\n",
    "            os.makedirs('ts_plots_assets')\n",
    "        path = os.path.join('ts_plots_assets', filename)\n",
    "        fig.savefig(path)\n",
    "        if show:\n",
    "            plt.show()\n",
    "\n",
    "    def diagnostic_check(self, alpha=None, return_stationarity=False):\n",
    "        alpha = alpha if alpha is not None else self.alpha\n",
    "        \n",
    "        # Augmented Dickey-Fuller test\n",
    "        adf_result = adfuller(self.data[self.target])\n",
    "        if adf_result[1] <= alpha:\n",
    "            adf_conclusion = \"The time series appears to be stationary based on the ADF test.\"\n",
    "            adf_guidance = \"You might not need to difference the time series. However, consider checking other diagnostics and plots to confirm.\"\n",
    "        else:\n",
    "            adf_conclusion = \"The time series appears to be non-stationary based on the ADF test. Differencing might be needed to make it stationary.\"\n",
    "            adf_guidance = \"Consider applying differencing or transformation to achieve stationarity.\"\n",
    "        \n",
    "        # Jarque-Bera test\n",
    "        jb_value, p_value = jarque_bera(self.data[self.target])\n",
    "        if p_value > alpha:\n",
    "            jb_conclusion = \"The time series seems to follow a normal distribution based on the Jarque-Bera test.\"\n",
    "            jb_guidance = \"The normality assumption holds. This is good if you plan to use models that assume normally distributed residuals.\"\n",
    "        else:\n",
    "            jb_conclusion = \"The time series does not appear to be normally distributed based on the Jarque-Bera test.\"\n",
    "            jb_guidance = \"Consider transforming the series or using models that do not assume normality.\"\n",
    "        \n",
    "        # KPSS test\n",
    "        kpss_value, kpss_p_value, _, kpss_crit = kpss(self.data[self.target])\n",
    "        if kpss_p_value > alpha:\n",
    "            kpss_conclusion = \"The time series appears to be stationary around a constant or trend based on the KPSS test.\"\n",
    "            kpss_guidance = \"The series might be stationary. However, consider other diagnostics to confirm.\"\n",
    "        else:\n",
    "            kpss_conclusion = \"The time series appears to be non-stationary based on the KPSS test. It might have a unit root.\"\n",
    "            kpss_guidance = \"Consider differencing or detrending the series to achieve stationarity.\"\n",
    "        \n",
    "        # Kolmogorov-Smirnov test\n",
    "        ks_value, ks_p_value = kstest(self.data[self.target], 'norm')\n",
    "        if ks_p_value > alpha:\n",
    "            ks_conclusion = \"The time series appears to follow a normal distribution based on the Kolmogorov-Smirnov test.\"\n",
    "            ks_guidance = \"The normality assumption holds, which is beneficial for certain statistical models.\"\n",
    "        else:\n",
    "            ks_conclusion = \"The time series does not seem to follow a normal distribution based on the Kolmogorov-Smirnov test.\"\n",
    "            ks_guidance = \"Consider transforming the series or using models that do not assume normality.\"\n",
    "    \n",
    "        # Create the results dictionary\n",
    "        results = {\n",
    "            'ADF': {\n",
    "                'Statistic': adf_result[0],\n",
    "                'p-value': adf_result[1],\n",
    "                'Critical Values': adf_result[4],\n",
    "                'Conclusion': adf_conclusion,\n",
    "                'Guidance': adf_guidance\n",
    "            },\n",
    "            'Jarque-Bera': {\n",
    "                'Statistic': jb_value,\n",
    "                'p-value': p_value,\n",
    "                'Conclusion': jb_conclusion,\n",
    "                'Guidance': jb_guidance\n",
    "            },\n",
    "            'KPSS': {\n",
    "                'Statistic': kpss_value,\n",
    "                'p-value': kpss_p_value,\n",
    "                'Critical Values': kpss_crit,\n",
    "                'Conclusion': kpss_conclusion,\n",
    "                'Guidance': kpss_guidance\n",
    "            },\n",
    "            'Kolmogorov-Smirnov': {\n",
    "                'Statistic': ks_value,\n",
    "                'p-value': ks_p_value,\n",
    "                'Conclusion': ks_conclusion,\n",
    "                'Guidance': ks_guidance\n",
    "            }\n",
    "        }\n",
    "    \n",
    "        # Check for Seasonality using ACF\n",
    "        lag_acf = acf(self.data[self.target], nlags=40)\n",
    "        lag_pacf = pacf(self.data[self.target], nlags=40, method='ols')\n",
    "    \n",
    "        # If there are significant peaks at regular intervals in ACF, we can suspect seasonality\n",
    "        seasonality_conclusion = \"Seasonality is likely present in the time series.\" if any(lag_acf > 0.2) else \"Seasonality is likely not present in the time series.\"\n",
    "    \n",
    "        # Now, update the results dictionary with the seasonality check\n",
    "        results['Seasonality'] = {\n",
    "            'Conclusion': seasonality_conclusion\n",
    "        }\n",
    "    \n",
    "        # Determine stationarity based on the tests\n",
    "        is_stationary = adf_result[1] <= alpha and kpss_p_value > alpha\n",
    "    \n",
    "        # Output or return results based on return_stationarity\n",
    "        if return_stationarity:\n",
    "            return is_stationary\n",
    "        else:\n",
    "            # Output to console\n",
    "            for test, result in results.items():\n",
    "                print(f\"--- {test} ---\")\n",
    "                for key, value in result.items():\n",
    "                    print(f\"{key}: {value}\")\n",
    "                print(\"\\n\")\n",
    "            return None\n",
    "\n",
    "    def auto_stationary(self, seasonal_period=None):\n",
    "        \"\"\"\n",
    "        Automatically choose the best method to make the series stationary based on its characteristics.\n",
    "    \n",
    "        Parameters:\n",
    "            seasonal_period (int, optional): The period of the seasonality, used for seasonal differencing.\n",
    "    \n",
    "        Returns:\n",
    "            pd.Series: The transformed time series.\n",
    "        \"\"\"\n",
    "        original_series = self.data[self.target]\n",
    "        \n",
    "        # Check for stationarity\n",
    "        if self.diagnostic_check(return_stationarity=True):\n",
    "            print(\"The series is already stationary.\")\n",
    "            return original_series\n",
    "        \n",
    "        # Try differencing\n",
    "        diff_series = original_series.diff().dropna()\n",
    "        if self.diagnostic_check(alpha=self.alpha, return_stationarity=True):\n",
    "            print(\"Differencing made the series stationary.\")\n",
    "            return diff_series\n",
    "        \n",
    "        # Try log transformation\n",
    "        log_series = np.log(original_series).dropna()\n",
    "        if self.diagnostic_check(alpha=self.alpha, return_stationarity=True):\n",
    "            print(\"Log transformation made the series stationary.\")\n",
    "            return log_series\n",
    "        \n",
    "        # Try square root transformation\n",
    "        sqrt_series = np.sqrt(original_series).dropna()\n",
    "        if self.diagnostic_check(alpha=self.alpha, return_stationarity=True):\n",
    "            print(\"Square root transformation made the series stationary.\")\n",
    "            return sqrt_series\n",
    "        \n",
    "        # If seasonal_period is provided, try seasonal differencing\n",
    "        if seasonal_period is not None:\n",
    "            seasonal_diff_series = original_series.diff(seasonal_period).dropna()\n",
    "            if self.diagnostic_check(alpha=self.alpha, return_stationarity=True):\n",
    "                print(\"Seasonal differencing made the series stationary.\")\n",
    "                return seasonal_diff_series\n",
    "        \n",
    "        print(\"Could not make the series stationary with the tried transformations.\")\n",
    "        return None\n",
    "\n",
    "    def make_stationary(self, method='auto', seasonal_period=None):\n",
    "        \"\"\"\n",
    "        Make the time series stationary.\n",
    "    \n",
    "        Parameters:\n",
    "            method (str): The method used to make the series stationary. Options are 'diff' (differencing), \n",
    "                          'seasonal_diff' (seasonal differencing), 'log' (log transformation), 'sqrt' (square root transformation),\n",
    "                          and 'auto' (automatically choose the best method).\n",
    "            seasonal_period (int, optional): The period of the seasonality, used for seasonal differencing.\n",
    "    \n",
    "        Returns:\n",
    "            pd.Series: The transformed time series.\n",
    "        \"\"\"\n",
    "        if method == 'auto':\n",
    "            # Providing recommendations based on the characteristics of the time series\n",
    "            return self.auto_stationary(seasonal_period)\n",
    "        elif method == 'diff':\n",
    "            return self.data[self.target].diff().dropna()\n",
    "        elif method == 'seasonal_diff' and seasonal_period:\n",
    "            return self.data[self.target].diff(seasonal_period).dropna()\n",
    "        elif method == 'log':\n",
    "            return np.log(self.data[self.target]).dropna()\n",
    "        elif method == 'sqrt':\n",
    "            return np.sqrt(self.data[self.target]).dropna()\n",
    "        else:\n",
    "            raise ValueError(\"Invalid method or missing seasonal_period for seasonal differencing.\")\n",
    "    \n",
    "\n",
    "    def visualize_stationarity(self, show_plot=True):\n",
    "        \"\"\"\n",
    "        Visualize the series before and after making it stationary.\n",
    "\n",
    "        Parameters:\n",
    "            show_plot (bool): Whether to display the plot.\n",
    "        \n",
    "        Returns:\n",
    "            matplotlib.Figure: The generated figure.\n",
    "        \"\"\"\n",
    "        fig, ax = plt.subplots(2, 1, figsize=(15, 10))\n",
    "\n",
    "        # Plot original data\n",
    "        self.original_data[self.target].plot(ax=ax[0], title='Original Series', color='blue')\n",
    "        \n",
    "        # Plot transformed data\n",
    "        self.data[self.target].plot(ax=ax[1], title='Transformed Series', color='green')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        self.save_and_show_plot(fig, 'stationarity_comparison.png', show=show_plot)\n",
    "        \n",
    "        return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f809680c-4b1b-489e-aa8e-e4291f51e77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the TimeSeriesAnalysis class\n",
    "tsa = TimeSeriesAnalysis1(btc_data, target='Close')\n",
    "\n",
    "# Check the diagnostics on the original series\n",
    "print(\"Diagnostics for Original Series:\")\n",
    "original_diagnostics = tsa.diagnostic_check()\n",
    "\n",
    "# Make the series stationary and update the data attribute\n",
    "stationary_series = tsa.make_stationary(method='log')\n",
    "tsa.data[tsa.target] = stationary_series\n",
    "\n",
    "# Visualize the series after transformation\n",
    "tsa.visualize_stationarity(show_plot=True)\n",
    "\n",
    "# Create a new instance with the stationary series\n",
    "tsa_stationary = TimeSeriesAnalysis1(pd.DataFrame({tsa.target: stationary_series}), target=tsa.target)\n",
    "\n",
    "# Check the diagnostics on the stationary series\n",
    "print(\"\\nDiagnostics for Stationary Series:\")\n",
    "stationary_diagnostics = tsa_stationary.diagnostic_check()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957b8e5f-a952-4f3c-8469-a8ad89ed5c14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f24149-5a29-4326-9bb3-acd1558a7260",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc960b4c-0039-4e3a-9e26-f8da6aa3c5e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c243452b-35d7-4667-8d67-b92868b4bdd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4b0ebd-9d9e-4d53-b7a7-038652868c8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bac0b4-1404-4ad9-bbf5-69f295b3477c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322491bf-2622-4843-9882-d61dc89f0956",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68b2af0-90bd-4406-90a7-cfaa28d53f0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d839be03-2eaa-475c-801f-54434a49d02d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1ccd72-693c-4b85-adbd-c866460372ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c1937a-d428-42e2-9fac-4ea4434f3d61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb222f1d-d4e2-4933-b366-594dbef492e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddeea9d-e7ab-4e5a-9282-27dbdfdce9bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dce67f-fbfe-42b6-87b1-612daf0ccda1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7107e1a-3e93-4ae7-90ae-b5148383fb94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dac498-f312-4edb-9291-a3a220b6dcd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0c268d-d1b8-4220-92e7-e9da36718031",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a24d4a-db61-402f-b02b-320b1293d1ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdab25ab-b07e-401e-b0e7-4e56c44a9c58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafb07a3-8893-4052-8c95-7aace9e03a19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9503bd-99a8-4c90-8984-8bde654a6208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78908cd-575a-4d58-b90c-bb020358a439",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49883c9-49d1-4715-b776-cd8897bc16f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf77c81-179e-49a4-be31-65910bbe1478",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b076288c-888a-4c20-8430-16b706875961",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ab6e57-d974-42b3-9244-0357cb61bff5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba72cd5-3329-4c2b-899c-0b88ffa7d34a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74430d6-553a-4cdb-9256-c8f8e1d81ba8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f46d6d6-be07-4939-9ca5-48a73817740f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb350c9-258e-4146-bbaa-283c49243a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "\n",
    "class TimeSeriesAnalysis:\n",
    "    \"\"\"\n",
    "    A class to perform various time series analysis tasks such as stationarity checks, volatility modeling, and decomposition.\n",
    "\n",
    "    Attributes:\n",
    "        data (pd.DataFrame): Time series data.\n",
    "        target (str): Target column for time series analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, target):\n",
    "        \"\"\"\n",
    "        Initialize the TimeSeriesAnalysis class.\n",
    "\n",
    "        Parameters:\n",
    "            data (pd.DataFrame): Time series data.\n",
    "            target (str): Target column for time series analysis.\n",
    "        \"\"\"\n",
    "        logger.info(\"Initializing TimeSeriesAnalysis class\")\n",
    "        if target not in data.columns:\n",
    "            raise ValueError(f\"'{target}' is not a column in the provided data.\")\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self.alpha = 0.05  \n",
    "\n",
    "    def check_autocorrelation(self):\n",
    "        \"\"\"\n",
    "        Check the autocorrelation of the time series using ACF and PACF plots.\n",
    "\n",
    "        Returns:\n",
    "            tuple: ACF and PACF figures.\n",
    "        \"\"\"\n",
    "        logger.info(\"Checking autocorrelation of the time series\")\n",
    "        fig, (ax1, ax2) = plt.subplots(1,2, figsize=(20,6))\n",
    "        plot_acf(self.data[self.target], lags=50, alpha=0.05, ax=ax1)\n",
    "        ax1.set_title(\"ACF for {}\".format(self.target))\n",
    "        plot_pacf(self.data[self.target], lags=50, alpha=0.05, method='ols', ax=ax2)\n",
    "        ax2.set_title(\"PACF for {}\".format(self.target))\n",
    "        self.save_and_show_plot(fig, 'autocorrelation.png')\n",
    "        return fig\n",
    "    \n",
    "    def decompose_time_series(self, model='additive', period=30, show=True):\n",
    "        \"\"\"\n",
    "        Decompose the time series data into trend, seasonal, and residual components.\n",
    "\n",
    "        Parameters:\n",
    "            model (str): The type of decomposition model ('additive' or 'multiplicative').\n",
    "            period (int): The period for seasonal decomposition.\n",
    "            show (bool): Whether to display the plot.\n",
    "        Returns:\n",
    "            fig (matplotlib.Figure): The figure object containing the decomposition plots.\n",
    "        \"\"\"\n",
    "        logger.info(\"Decomposing the time series\")\n",
    "        result = seasonal_decompose(self.data[self.target], model=model, period=period)\n",
    "\n",
    "        # Adjusting the figsize here\n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 12))\n",
    "\n",
    "        result.observed.plot(ax=ax1)\n",
    "        ax1.set_title('Observed')\n",
    "        result.trend.plot(ax=ax2)\n",
    "        ax2.set_title('Trend')\n",
    "        result.seasonal.plot(ax=ax3)\n",
    "        ax3.set_title('Seasonal')\n",
    "        result.resid.plot(ax=ax4)\n",
    "        ax4.set_title('Residual')\n",
    "\n",
    "        self.save_and_show_plot(fig, 'decompose.png', show=show)\n",
    "\n",
    "        return fig\n",
    "        \n",
    "    def save_and_show_plot(self, fig, filename, show=True):\n",
    "        \"\"\"\n",
    "        Utility method to save and display the plot.\n",
    "\n",
    "        Parameters:\n",
    "            fig (matplotlib.figure.Figure): The plot figure.\n",
    "            filename (str): Filename to save the plot.\n",
    "            show (bool, optional): Whether to display the plot. Default is True.\n",
    "        \"\"\"\n",
    "        if not os.path.exists('ts_plots_assets'):\n",
    "            os.makedirs('ts_plots_assets')\n",
    "        path = os.path.join('ts_plots_assets', filename)\n",
    "        fig.savefig(path)\n",
    "        logger.info(f\"Plot saved to: {path}\")\n",
    "        if show:\n",
    "            plt.show()\n",
    "\n",
    "    def diagnostic_check(self, alpha=None):\n",
    "        alpha = alpha if alpha is not None else self.alpha\n",
    "        \n",
    "        # Augmented Dickey-Fuller test\n",
    "        adf_result = adfuller(self.data[self.target])\n",
    "        if adf_result[1] <= alpha:\n",
    "            adf_conclusion = \"The time series appears to be stationary based on the ADF test.\"\n",
    "        else:\n",
    "            adf_conclusion = \"The time series appears to be non-stationary based on the ADF test. Differencing might be needed to make it stationary.\"\n",
    "        \n",
    "        # Jarque-Bera test\n",
    "        jb_value, p_value = jarque_bera(self.data[self.target])\n",
    "        if p_value > alpha:\n",
    "            jb_conclusion = \"The time series seems to follow a normal distribution based on the Jarque-Bera test.\"\n",
    "        else:\n",
    "            jb_conclusion = \"The time series does not appear to be normally distributed based on the Jarque-Bera test.\"\n",
    "        \n",
    "        # KPSS test\n",
    "        kpss_value, kpss_p_value, _, kpss_crit = kpss(self.data[self.target])\n",
    "        if kpss_p_value > alpha:\n",
    "            kpss_conclusion = \"The time series appears to be stationary around a constant or trend based on the KPSS test.\"\n",
    "        else:\n",
    "            kpss_conclusion = \"The time series appears to be non-stationary based on the KPSS test. It might have a unit root.\"\n",
    "        \n",
    "        # Kolmogorov-Smirnov test\n",
    "        ks_value, ks_p_value = kstest(self.data[self.target], 'norm')\n",
    "        if ks_p_value > alpha:\n",
    "            ks_conclusion = \"The time series appears to follow a normal distribution based on the Kolmogorov-Smirnov test.\"\n",
    "        else:\n",
    "            ks_conclusion = \"The time series does not seem to follow a normal distribution based on the Kolmogorov-Smirnov test.\"\n",
    "    \n",
    "        # Create the results dictionary first\n",
    "        results = {\n",
    "            'ADF': {\n",
    "                'Statistic': adf_result[0],\n",
    "                'p-value': adf_result[1],\n",
    "                'Critical Values': adf_result[4],\n",
    "                'Conclusion': adf_conclusion\n",
    "            },\n",
    "            'Jarque-Bera': {\n",
    "                'Statistic': jb_value,\n",
    "                'p-value': p_value,\n",
    "                'Conclusion': jb_conclusion\n",
    "            },\n",
    "            'KPSS': {\n",
    "                'Statistic': kpss_value,\n",
    "                'p-value': kpss_p_value,\n",
    "                'Critical Values': kpss_crit,\n",
    "                'Conclusion': kpss_conclusion\n",
    "            },\n",
    "            'Kolmogorov-Smirnov': {\n",
    "                'Statistic': ks_value,\n",
    "                'p-value': ks_p_value,\n",
    "                'Conclusion': ks_conclusion\n",
    "            }\n",
    "        }\n",
    "    \n",
    "        # Check for Seasonality using ACF\n",
    "        lag_acf = acf(self.data[self.target], nlags=40)\n",
    "        lag_pacf = pacf(self.data[self.target], nlags=40, method='ols')\n",
    "    \n",
    "        # If there are significant peaks at regular intervals in ACF, we can suspect seasonality\n",
    "        seasonality_conclusion = \"Seasonality is likely present in the time series.\" if any(lag_acf > 0.2) else \"Seasonality is likely not present in the time series.\"\n",
    "    \n",
    "        # Now, update the results dictionary with the seasonality check\n",
    "        results['Seasonality'] = {\n",
    "            'Conclusion': seasonality_conclusion\n",
    "        }\n",
    "    \n",
    "        # Output to console\n",
    "        for test, result in results.items():\n",
    "            print(f\"--- {test} ---\")\n",
    "            for key, value in result.items():\n",
    "                print(f\"{key}: {value}\")\n",
    "            print(\"\\n\")\n",
    "    \n",
    "        return results\n",
    "\n",
    "\n",
    "    def check_garch(self, p=1, q=1, return_residuals=False):\n",
    "        \"\"\"\n",
    "        Check volatility using GARCH model.\n",
    "\n",
    "        Parameters:\n",
    "            p (int, optional): The number of lag observations to include in the GARCH model. Default is 1.\n",
    "            q (int, optional): The number of lag forecast errors to include in the GARCH model. Default is 1.\n",
    "            return_residuals (bool, optional): Whether to return residuals of the GARCH model. Default is False.\n",
    "\n",
    "        Returns:\n",
    "            str or pd.Series: Summary of the GARCH model fit, or residuals if return_residuals is True.\n",
    "        \"\"\"\n",
    "        logger.info(\"Checking volatility of the time series\")\n",
    "        try:\n",
    "            model = arch_model(self.data[self.target], vol='Garch', p=p, q=q)\n",
    "            model_fit = model.fit(disp='off')\n",
    "            summary_str = model_fit.summary().as_text()\n",
    "            print(summary_str)\n",
    "            if return_residuals:\n",
    "                return model_fit.resid\n",
    "            else:\n",
    "                return summary_str\n",
    "        except Exception as e:\n",
    "            print(f\"Error encountered: {e}\")\n",
    "            return f\"Error encountered: {e}\"\n",
    "\n",
    "\n",
    "    def test_granger_causality(self, other_column, maxlag=30, verbose=False):\n",
    "        \"\"\"Test Granger Causality between target and another time series column.\n",
    "        \n",
    "        Parameters:\n",
    "            other_column (str): The name of the other column to test for Granger Causality.\n",
    "            maxlag (int): The maximum number of lags to consider for the test.\n",
    "            verbose (bool): Whether to display detailed output.\n",
    "        \n",
    "        Returns:\n",
    "            dict: A dictionary containing the Granger Causality test results.\n",
    "        \"\"\"\n",
    "        logger.info(\"Testing Granger causality\")\n",
    "        if other_column not in self.data.columns:\n",
    "            raise ValueError(f\"'{other_column}' is not a column in the provided data.\")\n",
    "        other_data = self.data[other_column].values\n",
    "        target_data = self.data[self.target].values\n",
    "        data = np.column_stack((target_data, other_data))\n",
    "        result = grangercausalitytests(data, maxlag=maxlag, verbose=verbose)\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    def concise_granger_output_table(self, granger_results):\n",
    "        \"\"\"Generate a concise report from the Granger Causality test results in a table format.\"\"\"\n",
    "        table_content = ['<table border=\"1\" style=\"border-collapse:collapse;\">']\n",
    "        lags = list(granger_results.keys())\n",
    "        for i in range(0, len(lags), 6):\n",
    "            table_content.append('<tr>')\n",
    "            for j in range(6):\n",
    "                if i + j < len(lags):\n",
    "                    lag = lags[i + j]\n",
    "                    test_statistics = granger_results[lag][0]\n",
    "                    cell_content = (f\"<b>Lag: {lag}</b><br>\"\n",
    "                                    f\"ssr_ftest: F={test_statistics['ssr_ftest'][0]:.4f}, p={test_statistics['ssr_ftest'][1]:.4f}<br>\"\n",
    "                                    f\"ssr_chi2test: chi2={test_statistics['ssr_chi2test'][0]:.4f}, p={test_statistics['ssr_chi2test'][1]:.4f}<br>\"\n",
    "                                    f\"lrtest: chi2={test_statistics['lrtest'][0]:.4f}, p={test_statistics['lrtest'][1]:.4f}<br>\"\n",
    "                                    f\"params_ftest: F={test_statistics['params_ftest'][0]:.4f}, p={test_statistics['params_ftest'][1]:.4f}\")\n",
    "                    table_content.append(f'<td style=\"padding: 8px; text-align: left;\">{cell_content}</td>')\n",
    "            table_content.append('</tr>')\n",
    "        table_content.append('</table>')\n",
    "        return \"\\n\".join(table_content)\n",
    "    \n",
    "    def make_stationary(self, method='diff', seasonal_period=None):\n",
    "        \"\"\"\n",
    "        Make the time series stationary.\n",
    "\n",
    "        Parameters:\n",
    "            method (str): The method used to make the series stationary. Options are 'diff' (differencing), \n",
    "                          'seasonal_diff' (seasonal differencing), 'log' (log transformation), 'sqrt' (square root transformation).\n",
    "            seasonal_period (int, optional): The period of the seasonality, used for seasonal differencing.\n",
    "\n",
    "        Returns:\n",
    "            pd.Series: The transformed time series.\n",
    "        \"\"\"\n",
    "        if method == 'diff':\n",
    "            return self.data[self.target].diff().dropna()\n",
    "        elif method == 'seasonal_diff' and seasonal_period:\n",
    "            return self.data[self.target].diff(seasonal_period).dropna()\n",
    "        elif method == 'log':\n",
    "            return np.log(self.data[self.target]).dropna()\n",
    "        elif method == 'sqrt':\n",
    "            return np.sqrt(self.data[self.target]).dropna()\n",
    "        else:\n",
    "            raise ValueError(\"Invalid method or missing seasonal_period for seasonal differencing.\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0b3766-b3c7-4911-a38b-6b780cfb1086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the TimeSeriesAnalysis class\n",
    "tsa = TimeSeriesAnalysis(btc_data, target='Close')\n",
    "\n",
    "# Check the diagnostics on the original series\n",
    "print(\"Diagnostics for Original Series:\")\n",
    "original_diagnostics = tsa.diagnostic_check()\n",
    "\n",
    "# Make the series stationary\n",
    "tsa.make_stationary(method = 'log')\n",
    "\n",
    "# Check the diagnostics on the stationary series\n",
    "print(\"\\nDiagnostics for Stationary Series:\")\n",
    "stationary_diagnostics = tsa.diagnostic_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0a5b11-5df9-44b2-bb05-753b8ffaa6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesAnalysis:\n",
    "    \"\"\"\n",
    "    A class to perform various time series analysis tasks such as stationarity checks, volatility modeling, and decomposition.\n",
    "\n",
    "    Attributes:\n",
    "        data (pd.DataFrame): Time series data.\n",
    "        target (str): Target column for time series analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, target):\n",
    "        \"\"\"\n",
    "        Initialize the TimeSeriesAnalysis class.\n",
    "\n",
    "        Parameters:\n",
    "            data (pd.DataFrame): Time series data.\n",
    "            target (str): Target column for time series analysis.\n",
    "        \"\"\"\n",
    "        if target not in data.columns:\n",
    "            raise ValueError(f\"'{target}' is not a column in the provided data.\")\n",
    "        self.original_data = data.copy()\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self.alpha = 0.05  \n",
    "\n",
    "    def check_autocorrelation(self, show_plot=True):\n",
    "        \"\"\"\n",
    "        Check the autocorrelation of the time series using ACF and PACF plots.\n",
    "\n",
    "        Returns:\n",
    "            tuple: ACF and PACF figures.\n",
    "        \"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1,2, figsize=(20,6))\n",
    "        plot_acf(self.data[self.target], lags=50, alpha=0.05, ax=ax1)\n",
    "        ax1.set_title(\"ACF for {}\".format(self.target))\n",
    "        plot_pacf(self.data[self.target], lags=50, alpha=0.05, method='ols', ax=ax2)\n",
    "        ax2.set_title(\"PACF for {}\".format(self.target))\n",
    "        self.save_and_show_plot(fig, 'autocorrelation.png', show=show_plot)\n",
    "        return fig\n",
    "    \n",
    "    def decompose_time_series(self, model='additive', period=None, show=True):\n",
    "        \"\"\"\n",
    "        Decompose the time series data into trend, seasonal, and residual components.\n",
    "    \n",
    "        Parameters:\n",
    "            model (str): The type of decomposition model ('additive' or 'multiplicative').\n",
    "            period (int): The period for seasonal decomposition. If None, it will be inferred.\n",
    "            show (bool): Whether to display the plot.\n",
    "        Returns:\n",
    "            dict: A dictionary containing decomposition components and guidance.\n",
    "        \"\"\"\n",
    "        logger.info(\"Decomposing the time series\")\n",
    "        if period is None:\n",
    "            # Attempt to infer the seasonal period\n",
    "            period = self.infer_seasonal_period()\n",
    "            \n",
    "        result = seasonal_decompose(self.data[self.target], model=model, period=period)\n",
    "    \n",
    "        # Adjusting the figsize here\n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 12))\n",
    "    \n",
    "        result.observed.plot(ax=ax1)\n",
    "        ax1.set_title('Observed')\n",
    "        result.trend.plot(ax=ax2)\n",
    "        ax2.set_title('Trend')\n",
    "        result.seasonal.plot(ax=ax3)\n",
    "        ax3.set_title('Seasonal')\n",
    "        result.resid.plot(ax=ax4)\n",
    "        ax4.set_title('Residual')\n",
    "    \n",
    "        self.save_and_show_plot(fig, 'decompose.png', show=show)\n",
    "        \n",
    "        # Analyzing the decomposition results\n",
    "        guidance = self.analyze_decomposition(result)\n",
    "        \n",
    "        # Returning the decomposition components and guidance\n",
    "        return {\n",
    "            'Observed': result.observed,\n",
    "            'Trend': result.trend,\n",
    "            'Seasonal': result.seasonal,\n",
    "            'Residual': result.resid,\n",
    "            'Guidance': guidance\n",
    "        }\n",
    "\n",
    "    def analyze_decomposition(self, decomposition_result):\n",
    "        \"\"\"\n",
    "        Analyze the time series decomposition results and provide guidance.\n",
    "    \n",
    "        Parameters:\n",
    "            decomposition_result (DecomposeResult): The result from seasonal decomposition.\n",
    "    \n",
    "        Returns:\n",
    "            str: Guidance based on the decomposition results.\n",
    "        \"\"\"\n",
    "        guidance = \"\"\n",
    "        \n",
    "        # Analyzing the trend component\n",
    "        if decomposition_result.trend.isnull().sum() > 0:\n",
    "            guidance += \"The trend component has missing values at the boundaries. Consider using a different decomposition method or filling the missing values.\\n\"\n",
    "        else:\n",
    "            trend_strength = np.nanmean(np.abs(decomposition_result.trend - np.nanmean(decomposition_result.trend)))\n",
    "            if trend_strength > 0.05 * np.nanmean(decomposition_result.observed):\n",
    "                guidance += \"The trend component is strong. Consider detrending the series if you intend to use models that assume stationarity.\\n\"\n",
    "            else:\n",
    "                guidance += \"The trend component is weak, indicating a relatively stable mean over time.\\n\"\n",
    "                \n",
    "        # Analyzing the seasonal component\n",
    "        seasonal_strength = np.nanmean(np.abs(decomposition_result.seasonal - np.nanmean(decomposition_result.seasonal)))\n",
    "        if seasonal_strength > 0.05 * np.nanmean(decomposition_result.observed):\n",
    "            guidance += \"The seasonal component is strong. Consider seasonal adjustment or using models that can handle seasonality.\\n\"\n",
    "        else:\n",
    "            guidance += \"The seasonal component is weak, indicating that seasonality may not be a significant factor.\\n\"\n",
    "            \n",
    "        # Analyzing the residual component\n",
    "        if decomposition_result.resid.isnull().sum() > 0:\n",
    "            guidance += \"The residual component has missing values at the boundaries. Consider using a different decomposition method or filling the missing values.\\n\"\n",
    "        else:\n",
    "            if np.nanstd(decomposition_result.resid) > 0.05 * np.nanmean(decomposition_result.observed):\n",
    "                guidance += \"The residual component shows variability. Consider further analysis to identify any remaining patterns or anomalies.\\n\"\n",
    "            else:\n",
    "                guidance += \"The residual component is relatively stable, indicating that most of the patterns have been captured by the trend and seasonal components.\\n\"\n",
    "                \n",
    "        return guidance\n",
    "        \n",
    "    def save_and_show_plot(self, fig, filename, show=True):\n",
    "        \"\"\"\n",
    "        Utility method to save and display the plot.\n",
    "\n",
    "        Parameters:\n",
    "            fig (matplotlib.figure.Figure): The plot figure.\n",
    "            filename (str): Filename to save the plot.\n",
    "            show (bool, optional): Whether to display the plot. Default is True.\n",
    "        \"\"\"\n",
    "        if not os.path.exists('ts_plots_assets'):\n",
    "            os.makedirs('ts_plots_assets')\n",
    "        path = os.path.join('ts_plots_assets', filename)\n",
    "        fig.savefig(path)\n",
    "        if show:\n",
    "            plt.show()\n",
    "\n",
    "    def diagnostic_check(self, alpha=None):\n",
    "        alpha = alpha if alpha is not None else self.alpha\n",
    "        \n",
    "        # Augmented Dickey-Fuller test\n",
    "        adf_result = adfuller(self.data[self.target])\n",
    "        if adf_result[1] <= alpha:\n",
    "            adf_conclusion = \"The time series appears to be stationary based on the ADF test.\"\n",
    "            adf_guidance = \"You might not need to difference the time series. However, consider checking other diagnostics and plots to confirm.\"\n",
    "        else:\n",
    "            adf_conclusion = \"The time series appears to be non-stationary based on the ADF test. Differencing might be needed to make it stationary.\"\n",
    "            adf_guidance = \"Consider applying differencing or transformation to achieve stationarity.\"\n",
    "        \n",
    "        # Jarque-Bera test\n",
    "        jb_value, p_value = jarque_bera(self.data[self.target])\n",
    "        if p_value > alpha:\n",
    "            jb_conclusion = \"The time series seems to follow a normal distribution based on the Jarque-Bera test.\"\n",
    "            jb_guidance = \"The normality assumption holds. This is good if you plan to use models that assume normally distributed residuals.\"\n",
    "        else:\n",
    "            jb_conclusion = \"The time series does not appear to be normally distributed based on the Jarque-Bera test.\"\n",
    "            jb_guidance = \"Consider transforming the series or using models that do not assume normality.\"\n",
    "        \n",
    "        # KPSS test\n",
    "        kpss_value, kpss_p_value, _, kpss_crit = kpss(self.data[self.target])\n",
    "        if kpss_p_value > alpha:\n",
    "            kpss_conclusion = \"The time series appears to be stationary around a constant or trend based on the KPSS test.\"\n",
    "            kpss_guidance = \"The series might be stationary. However, consider other diagnostics to confirm.\"\n",
    "        else:\n",
    "            kpss_conclusion = \"The time series appears to be non-stationary based on the KPSS test. It might have a unit root.\"\n",
    "            kpss_guidance = \"Consider differencing or detrending the series to achieve stationarity.\"\n",
    "        \n",
    "        # Kolmogorov-Smirnov test\n",
    "        ks_value, ks_p_value = kstest(self.data[self.target], 'norm')\n",
    "        if ks_p_value > alpha:\n",
    "            ks_conclusion = \"The time series appears to follow a normal distribution based on the Kolmogorov-Smirnov test.\"\n",
    "            ks_guidance = \"The normality assumption holds, which is beneficial for certain statistical models.\"\n",
    "        else:\n",
    "            ks_conclusion = \"The time series does not seem to follow a normal distribution based on the Kolmogorov-Smirnov test.\"\n",
    "            ks_guidance = \"Consider transforming the series or using models that do not assume normality.\"\n",
    "    \n",
    "        # Create the results dictionary\n",
    "        results = {\n",
    "            'ADF': {\n",
    "                'Statistic': adf_result[0],\n",
    "                'p-value': adf_result[1],\n",
    "                'Critical Values': adf_result[4],\n",
    "                'Conclusion': adf_conclusion,\n",
    "                'Guidance': adf_guidance\n",
    "            },\n",
    "            'Jarque-Bera': {\n",
    "                'Statistic': jb_value,\n",
    "                'p-value': p_value,\n",
    "                'Conclusion': jb_conclusion,\n",
    "                'Guidance': jb_guidance\n",
    "            },\n",
    "            'KPSS': {\n",
    "                'Statistic': kpss_value,\n",
    "                'p-value': kpss_p_value,\n",
    "                'Critical Values': kpss_crit,\n",
    "                'Conclusion': kpss_conclusion,\n",
    "                'Guidance': kpss_guidance\n",
    "            },\n",
    "            'Kolmogorov-Smirnov': {\n",
    "                'Statistic': ks_value,\n",
    "                'p-value': ks_p_value,\n",
    "                'Conclusion': ks_conclusion,\n",
    "                'Guidance': ks_guidance\n",
    "            }\n",
    "        }\n",
    "    \n",
    "        # Check for Seasonality using ACF\n",
    "        lag_acf = acf(self.data[self.target], nlags=40)\n",
    "        lag_pacf = pacf(self.data[self.target], nlags=40, method='ols')\n",
    "    \n",
    "        # If there are significant peaks at regular intervals in ACF, we can suspect seasonality\n",
    "        seasonality_conclusion = \"Seasonality is likely present in the time series.\" if any(lag_acf > 0.2) else \"Seasonality is likely not present in the time series.\"\n",
    "    \n",
    "        # Now, update the results dictionary with the seasonality check\n",
    "        results['Seasonality'] = {\n",
    "            'Conclusion': seasonality_conclusion\n",
    "        }\n",
    "    \n",
    "        # Output to console\n",
    "        for test, result in results.items():\n",
    "            print(f\"--- {test} ---\")\n",
    "            for key, value in result.items():\n",
    "                print(f\"{key}: {value}\")\n",
    "            print(\"\\n\")\n",
    "    \n",
    "        return results\n",
    "\n",
    "    def make_stationary(self, method='diff', seasonal_period=None):\n",
    "        \"\"\"\n",
    "        Make the time series stationary.\n",
    "\n",
    "        Parameters:\n",
    "            method (str): The method used to make the series stationary. Options are 'diff' (differencing), \n",
    "                          'seasonal_diff' (seasonal differencing), 'log' (log transformation), 'sqrt' (square root transformation).\n",
    "            seasonal_period (int, optional): The period of the seasonality, used for seasonal differencing.\n",
    "\n",
    "        Returns:\n",
    "            pd.Series: The transformed time series.\n",
    "        \"\"\"\n",
    "        if method == 'diff':\n",
    "            self.data[self.target] = self.data[self.target].diff().dropna()\n",
    "        elif method == 'seasonal_diff' and seasonal_period:\n",
    "            self.data[self.target] = self.data[self.target].diff(seasonal_period).dropna()\n",
    "        elif method == 'log':\n",
    "            self.data[self.target] = np.log(self.data[self.target]).dropna()\n",
    "        elif method == 'sqrt':\n",
    "            self.data[self.target] = np.sqrt(self.data[self.target]).dropna()\n",
    "        else:\n",
    "            raise ValueError(\"Invalid method or missing seasonal_period for seasonal differencing.\")\n",
    "        return self.data[self.target]\n",
    "\n",
    "    def visualize_stationarity(self, show_plot=True):\n",
    "        \"\"\"\n",
    "        Visualize the series before and after making it stationary.\n",
    "\n",
    "        Parameters:\n",
    "            show_plot (bool): Whether to display the plot.\n",
    "        \n",
    "        Returns:\n",
    "            matplotlib.Figure: The generated figure.\n",
    "        \"\"\"\n",
    "        fig, ax = plt.subplots(2, 1, figsize=(15, 10))\n",
    "\n",
    "        # Plot original data\n",
    "        self.original_data[self.target].plot(ax=ax[0], title='Original Series', color='blue')\n",
    "        \n",
    "        # Plot transformed data\n",
    "        self.data[self.target].plot(ax=ax[1], title='Transformed Series', color='green')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        self.save_and_show_plot(fig, 'stationarity_comparison.png', show=show_plot)\n",
    "        \n",
    "        return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16a98e1-f9e5-4e11-ab3e-3a14db0b3732",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Instantiate the TimeSeriesAnalysis class\n",
    "tsa = TimeSeriesAnalysis(btc_data, target='Close')\n",
    "# Check the diagnostics on the original series\n",
    "print(\"Diagnostics for Original Series:\")\n",
    "original_diagnostics = tsa.diagnostic_check()\n",
    "\n",
    "# Make the series stationary\n",
    "tsa.make_stationary(method='log')\n",
    "\n",
    "# Visualize the series after transformation\n",
    "tsa.visualize_stationarity(show_plot=True)\n",
    "\n",
    "# Check the diagnostics on the stationary series\n",
    "print(\"\\nDiagnostics for Stationary Series:\")\n",
    "stationary_diagnostics = tsa.diagnostic_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898a500f-4173-48a1-aaa5-e10f091a8d85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adf5f48-56d5-4484-9ddd-f46e93fe78eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb75e87-f5f3-4891-bd26-96430d2d49da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4cc16c-85df-47d9-9699-e12834d26f23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dc1545-4350-4049-9489-9dc44595a995",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714db0f1-868b-42ff-a33a-8277a37f7da7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab96df45-e518-45ff-9b94-82603e907021",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b94587-35af-4268-9957-cbc2b8e97991",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d58364-8d35-4cd1-8466-24a1f0400b8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b462e2fc-5da0-4355-bcd9-ec2f73c0c3ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0d94e3-9e8a-4788-825d-195175083627",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125f3c25-2312-48f0-83d3-41ca6f97484b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44be82f5-19ac-4f9d-b1fd-968516bfe4ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51623759-8bb0-4714-b451-468e9de6cb0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28b187d-ccf6-4f81-a881-837479e6e335",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58261a2-bb04-4111-b834-dc51e590bac1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba79c631-f60c-4805-9092-ffafd580d159",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872476da-1030-4118-a269-842343f4b4f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93345bc8-7ad8-4b13-9fbf-8252c69fa9b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4833694-4e98-4719-a0d5-4219cf82d852",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943a912e-bbfc-4dfc-836d-b8419c260895",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4684a285-2513-45f8-a8dc-55f4e84cdfa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3e629f-c001-4df2-9bfb-3df8bb654dd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3b7b9f-bc46-4357-bb79-b790b3a01e09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47107a96-c392-4b6e-9b19-87671843119c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7969b4-ce32-45fe-9f18-e2fb751cc1a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6302ee-c833-44d5-b09c-76437f27e704",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620d406a-a59c-46c6-9349-395a1e14137e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66ca183-53d1-4270-bc34-636b77bdcb9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0274e2-c3c4-4dcb-bf9e-75ad6879639b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5efb788-74c9-4a6e-b9ac-4c650ef0af1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4516970f-5bea-4be8-9895-66153e4451d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0c1215-aefd-4acc-979f-ef62f07c093b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
